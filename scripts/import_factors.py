#!/usr/bin/env python
"""
å› å­åº“å¯¼å…¥ä¸éªŒè¯è„šæœ¬

åŠŸèƒ½:
1. åŠ è½½æ‰€æœ‰é¢„å®šä¹‰å› å­ (300+)
   - ç»å…¸å› å­ (Barra/æŠ€æœ¯/åŸºæœ¬é¢/é‡ä»·): 25ä¸ª
   - Alpha158 (Qlib): 50ä¸ª
   - Alpha360 (Qlib): 27ä¸ª
   - WorldQuant 101: 29ä¸ª
   - å›½æ³°å›å®‰ 191: 30ä¸ª
   - Academic Premia: 18ä¸ª
2. å¯¼å…¥åˆ°Milvuså‘é‡æ•°æ®åº“ (ç”¨äºRAGæ£€ç´¢)
3. å¯¼å…¥åˆ°Neo4jçŸ¥è¯†å›¾è°± (ç”¨äºGraphRAG)
4. å¯¼å…¥åˆ°Redisç¼“å­˜ (ç”¨äºå¿«é€Ÿè®¿é—®)
5. ä½¿ç”¨Qlibæ•°æ®éªŒè¯å› å­IC

ä½¿ç”¨æ–¹æ³•:
    # å¯¼å…¥æ‰€æœ‰å› å­åˆ°æ•°æ®åº“
    python scripts/import_factors.py --import-all
    
    # åªéªŒè¯å› å­IC (ä¸å¯¼å…¥)
    python scripts/import_factors.py --validate-only
    
    # å¯¼å…¥å¹¶éªŒè¯
    python scripts/import_factors.py --import-all --validate
    
    # åªå¯¼å…¥å‰Nä¸ªå› å­ (æµ‹è¯•)
    python scripts/import_factors.py --import-all --limit 10
    
    # æŒ‰æ¥æºå¯¼å…¥
    python scripts/import_factors.py --import-milvus
    python scripts/import_factors.py --import-neo4j
    python scripts/import_factors.py --import-redis
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import sys
import time
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import asdict

import numpy as np
import pandas as pd

warnings.filterwarnings('ignore')

# è®¾ç½®é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT.parent))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================
# å› å­åŠ è½½
# ============================================================

def load_all_factors() -> List[Dict]:
    """åŠ è½½æ‰€æœ‰é¢„å®šä¹‰å› å­"""
    # ç›´æ¥ä»factorsæ¨¡å—å¯¼å…¥ï¼Œé¿å…è§¦å‘alpha_agenté¡¶å±‚çš„å¯é€‰ä¾èµ–æ£€æŸ¥
    from alpha_agent.factors.classic_factors import ALL_CLASSIC_FACTORS, ClassicFactor
    from alpha_agent.factors.alpha158 import ALPHA158_FACTORS
    from alpha_agent.factors.alpha360 import ALPHA360_FACTORS
    from alpha_agent.factors.worldquant101 import WORLDQUANT_101_FACTORS
    from alpha_agent.factors.gtja191 import GTJA191_FACTORS
    from alpha_agent.factors.academic_premia import ACADEMIC_PREMIA_FACTORS
    
    def factor_to_dict(f: ClassicFactor, source: str = 'classic') -> Dict:
        """å°†ClassicFactorè½¬æ¢ä¸ºå­—å…¸"""
        return {
            'id': f.id,
            'name': f.name,
            'name_en': getattr(f, 'name_en', f.name),
            'code': f.code,
            'category': f.category.value if hasattr(f.category, 'value') else str(f.category),
            'source': source,
            'description': f.description,
            'tags': f.tags,
            'ic': f.historical_ic,
            'icir': f.historical_icir,
            'reference': f.reference,
            'logic': f.logic,
            'author': getattr(f, 'author', ''),
            'year': getattr(f, 'year', 0),
        }
    
    all_factors = []
    
    # 1. ç»å…¸å› å­ (Barra/æŠ€æœ¯/åŸºæœ¬é¢/é‡ä»·)
    logger.info("åŠ è½½ç»å…¸å› å­...")
    for f in ALL_CLASSIC_FACTORS:
        all_factors.append(factor_to_dict(f, 'classic'))
    logger.info(f"  ç»å…¸å› å­: {len(ALL_CLASSIC_FACTORS)}")
    
    # 2. Alpha158 (Qlib)
    logger.info("åŠ è½½Alpha158å› å­...")
    for f in ALPHA158_FACTORS:
        all_factors.append(factor_to_dict(f, 'qlib_alpha158'))
    logger.info(f"  Alpha158: {len(ALPHA158_FACTORS)}")
    
    # 3. Alpha360 (Qlib)
    logger.info("åŠ è½½Alpha360å› å­...")
    for f in ALPHA360_FACTORS:
        all_factors.append(factor_to_dict(f, 'qlib_alpha360'))
    logger.info(f"  Alpha360: {len(ALPHA360_FACTORS)}")
    
    # 4. WorldQuant 101
    logger.info("åŠ è½½WorldQuant101å› å­...")
    for f in WORLDQUANT_101_FACTORS:
        all_factors.append(factor_to_dict(f, 'worldquant101'))
    logger.info(f"  WorldQuant101: {len(WORLDQUANT_101_FACTORS)}")
    
    # 5. å›½æ³°å›å®‰ 191
    logger.info("åŠ è½½å›½æ³°å›å®‰191å› å­...")
    for f in GTJA191_FACTORS:
        all_factors.append(factor_to_dict(f, 'gtja191'))
    logger.info(f"  å›½æ³°å›å®‰191: {len(GTJA191_FACTORS)}")
    
    # 6. Academic Premia å­¦æœ¯æº¢ä»·
    logger.info("åŠ è½½Academic Premiaå› å­...")
    for f in ACADEMIC_PREMIA_FACTORS:
        all_factors.append(factor_to_dict(f, 'academic_premia'))
    logger.info(f"  Academic Premia: {len(ACADEMIC_PREMIA_FACTORS)}")
    
    logger.info(f"âœ… æ€»è®¡åŠ è½½ {len(all_factors)} ä¸ªå› å­")
    return all_factors


# ============================================================
# Milvuså¯¼å…¥
# ============================================================

def import_to_milvus(factors: List[Dict], batch_size: int = 50) -> Tuple[int, int]:
    """å¯¼å…¥å› å­åˆ°Milvuså‘é‡æ•°æ®åº“"""
    from alpha_agent.memory.vector_store import MilvusStore, FactorRecord, MILVUS_AVAILABLE
    
    if not MILVUS_AVAILABLE:
        logger.error("âŒ Milvus SDKæœªå®‰è£…: pip install pymilvus")
        return 0, len(factors)
    
    logger.info("\n" + "="*50)
    logger.info("å¯¼å…¥åˆ°Milvuså‘é‡æ•°æ®åº“")
    logger.info("="*50)
    
    try:
        store = MilvusStore(collection_name="alpha_factors")
        if not store.connect():
            logger.error("âŒ Milvusè¿æ¥å¤±è´¥")
            return 0, len(factors)
        
        # ç¡®ä¿é›†åˆå­˜åœ¨
        store.create_collection()
        
        success_count = 0
        fail_count = 0
        
        # æ‰¹é‡å¯¼å…¥
        for i in range(0, len(factors), batch_size):
            batch = factors[i:i+batch_size]
            
            for f in batch:
                try:
                    # åˆ›å»ºFactorRecord
                    record = FactorRecord(
                        factor_id=f.get('id', ''),
                        name=f.get('name', ''),
                        code=f.get('code', ''),
                        description=f.get('description', ''),
                        ic=f.get('ic', 0),
                        icir=f.get('icir', 0),
                        status='active',
                        tags=f.get('tags', []),
                    )
                    
                    # æ’å…¥ (ä¼šè‡ªåŠ¨ç”Ÿæˆembedding)
                    store.insert(record)
                    success_count += 1
                    
                except Exception as e:
                    logger.warning(f"  å¯¼å…¥å¤±è´¥ {f.get('id')}: {e}")
                    fail_count += 1
            
            logger.info(f"  è¿›åº¦: {min(i+batch_size, len(factors))}/{len(factors)}")
        
        logger.info(f"âœ… Milvuså¯¼å…¥å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
        return success_count, fail_count
        
    except Exception as e:
        logger.error(f"âŒ Milvuså¯¼å…¥å¤±è´¥: {e}")
        return 0, len(factors)


def import_to_milvus_simple(factors: List[Dict]) -> Tuple[int, int]:
    """ç®€åŒ–ç‰ˆMilvuså¯¼å…¥ (ä¸ä½¿ç”¨embedding)"""
    try:
        from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
        
        logger.info("\n" + "="*50)
        logger.info("å¯¼å…¥åˆ°Milvus (ç®€åŒ–æ¨¡å¼)")
        logger.info("="*50)
        
        # è¿æ¥
        connections.connect('default', host='localhost', port='19530')
        logger.info("âœ… Milvusè¿æ¥æˆåŠŸ")
        
        collection_name = "alpha_factors"
        
        # åˆ é™¤æ—§é›†åˆ
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)
            logger.info(f"  åˆ é™¤æ—§é›†åˆ: {collection_name}")
        
        # åˆ›å»ºSchema
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="factor_id", dtype=DataType.VARCHAR, max_length=128),
            FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=256),
            FieldSchema(name="name_en", dtype=DataType.VARCHAR, max_length=256),
            FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=64),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=64),  # å› å­æ¥æº
            FieldSchema(name="code", dtype=DataType.VARCHAR, max_length=4096),
            FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=1024),
            FieldSchema(name="ic", dtype=DataType.FLOAT),
            FieldSchema(name="icir", dtype=DataType.FLOAT),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128),  # ç®€åŒ–embedding
        ]
        
        schema = CollectionSchema(fields, description="Alphaå› å­åº“")
        collection = Collection(collection_name, schema)
        logger.info(f"  åˆ›å»ºé›†åˆ: {collection_name}")
        
        # å‡†å¤‡æ•°æ®
        factor_ids = []
        names = []
        names_en = []
        categories = []
        sources = []
        codes = []
        descriptions = []
        ics = []
        icirs = []
        embeddings = []
        
        for f in factors:
            factor_ids.append(f.get('id', '')[:128])
            names.append(f.get('name', '')[:256])
            names_en.append(f.get('name_en', f.get('name', ''))[:256])
            categories.append(f.get('category', '')[:64])
            sources.append(f.get('source', 'unknown')[:64])
            
            # æˆªæ–­code
            code = f.get('code', '')
            if len(code) > 4096:
                code = code[:4090] + "..."
            codes.append(code)
            
            # æˆªæ–­description
            desc = f.get('description', '')
            if len(desc) > 1024:
                desc = desc[:1020] + "..."
            descriptions.append(desc)
            
            ics.append(float(f.get('ic', 0) or 0))
            icirs.append(float(f.get('icir', 0) or 0))
            
            # ç®€å•hashä½œä¸ºembedding (åç»­å¯æ›¿æ¢ä¸ºçœŸæ­£çš„å‘é‡)
            code_hash = hash(f.get('code', '') + f.get('description', ''))
            embedding = [float((code_hash >> i) & 1) for i in range(128)]
            embeddings.append(embedding)
        
        # æ‰¹é‡æ’å…¥
        data = [factor_ids, names, names_en, categories, sources, codes, descriptions, ics, icirs, embeddings]
        collection.insert(data)
        
        # åˆ›å»ºç´¢å¼•
        index_params = {
            "metric_type": "L2",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 128}
        }
        collection.create_index("embedding", index_params)
        collection.load()
        
        logger.info(f"âœ… Milvuså¯¼å…¥å®Œæˆ: {len(factors)} ä¸ªå› å­")
        
        connections.disconnect('default')
        return len(factors), 0
        
    except Exception as e:
        logger.error(f"âŒ Milvuså¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0, len(factors)


# ============================================================
# Neo4jå¯¼å…¥
# ============================================================

def import_to_neo4j(factors: List[Dict]) -> Tuple[int, int]:
    """å¯¼å…¥å› å­åˆ°Neo4jçŸ¥è¯†å›¾è°±"""
    try:
        from neo4j import GraphDatabase
        
        logger.info("\n" + "="*50)
        logger.info("å¯¼å…¥åˆ°Neo4jçŸ¥è¯†å›¾è°±")
        logger.info("="*50)
        
        driver = GraphDatabase.driver(
            'bolt://localhost:7687',
            auth=('neo4j', 'password')
        )
        
        success_count = 0
        fail_count = 0
        
        with driver.session() as session:
            # æ¸…ç©ºæ—§æ•°æ® (å¯é€‰)
            session.run("MATCH (n:Factor) DETACH DELETE n")
            session.run("MATCH (n:Category) DETACH DELETE n")
            logger.info("  æ¸…ç©ºæ—§æ•°æ®")
            
            # åˆ›å»ºåˆ†ç±»èŠ‚ç‚¹
            categories = set(f.get('category', 'unknown') for f in factors)
            for cat in categories:
                session.run(
                    "MERGE (c:Category {name: $name})",
                    name=cat
                )
            logger.info(f"  åˆ›å»ºåˆ†ç±»èŠ‚ç‚¹: {len(categories)}")
            
            # åˆ›å»ºå› å­èŠ‚ç‚¹å’Œå…³ç³»
            for f in factors:
                try:
                    session.run("""
                        MERGE (f:Factor {factor_id: $factor_id})
                        SET f.name = $name,
                            f.code = $code,
                            f.ic = $ic,
                            f.icir = $icir,
                            f.source = $source,
                            f.description = $description
                        WITH f
                        MATCH (c:Category {name: $category})
                        MERGE (f)-[:BELONGS_TO]->(c)
                    """,
                        factor_id=f.get('id', ''),
                        name=f.get('name', ''),
                        code=f.get('code', '')[:2000],  # é™åˆ¶é•¿åº¦
                        ic=float(f.get('ic', 0) or 0),
                        icir=float(f.get('icir', 0) or 0),
                        source=f.get('source', ''),
                        description=f.get('description', '')[:500],
                        category=f.get('category', 'unknown'),
                    )
                    success_count += 1
                except Exception as e:
                    logger.warning(f"  Neo4jå¯¼å…¥å¤±è´¥ {f.get('id')}: {e}")
                    fail_count += 1
            
            # åˆ›å»ºç´¢å¼•
            session.run("CREATE INDEX factor_id_index IF NOT EXISTS FOR (f:Factor) ON (f.factor_id)")
            session.run("CREATE INDEX factor_name_index IF NOT EXISTS FOR (f:Factor) ON (f.name)")
        
        driver.close()
        logger.info(f"âœ… Neo4jå¯¼å…¥å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
        return success_count, fail_count
        
    except Exception as e:
        logger.error(f"âŒ Neo4jå¯¼å…¥å¤±è´¥: {e}")
        return 0, len(factors)


# ============================================================
# Redisç¼“å­˜
# ============================================================

def import_to_redis(factors: List[Dict]) -> Tuple[int, int]:
    """å¯¼å…¥å› å­åˆ°Redisç¼“å­˜"""
    try:
        import redis
        
        logger.info("\n" + "="*50)
        logger.info("å¯¼å…¥åˆ°Redisç¼“å­˜")
        logger.info("="*50)
        
        r = redis.Redis(host='localhost', port=6379, decode_responses=True)
        r.ping()
        
        # æ¸…ç©ºæ—§æ•°æ®
        keys = r.keys("factor:*")
        if keys:
            r.delete(*keys)
        r.delete("factor_index")
        
        success_count = 0
        
        for f in factors:
            factor_id = f.get('id', '')
            key = f"factor:{factor_id}"
            
            # å­˜å‚¨å› å­æ•°æ®
            r.hset(key, mapping={
                'name': f.get('name', ''),
                'category': f.get('category', ''),
                'code': f.get('code', '')[:4000],
                'ic': str(f.get('ic', 0)),
                'icir': str(f.get('icir', 0)),
                'source': f.get('source', ''),
            })
            
            # æ·»åŠ åˆ°ç´¢å¼•
            r.sadd("factor_index", factor_id)
            
            # æŒ‰åˆ†ç±»ç´¢å¼•
            r.sadd(f"category:{f.get('category', 'unknown')}", factor_id)
            
            success_count += 1
        
        logger.info(f"âœ… Rediså¯¼å…¥å®Œæˆ: {success_count} ä¸ªå› å­")
        return success_count, 0
        
    except Exception as e:
        logger.error(f"âŒ Rediså¯¼å…¥å¤±è´¥: {e}")
        return 0, len(factors)


# ============================================================
# å› å­éªŒè¯
# ============================================================

def validate_factors(factors: List[Dict], limit: int = None) -> pd.DataFrame:
    """éªŒè¯å› å­IC"""
    logger.info("\n" + "="*50)
    logger.info("å› å­ICéªŒè¯")
    logger.info("="*50)
    
    # åˆå§‹åŒ–Qlib
    try:
        import qlib
        from qlib.config import REG_CN
        from qlib.data import D
        
        provider_uri = os.path.expanduser("~/.qlib/qlib_data/cn_data")
        qlib.init(provider_uri=provider_uri, region=REG_CN)
        logger.info("âœ… Qlibåˆå§‹åŒ–æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"âŒ Qlibåˆå§‹åŒ–å¤±è´¥: {e}")
        return pd.DataFrame()
    
    # åŠ è½½æ•°æ®
    logger.info("ğŸ“Š åŠ è½½å¸‚åœºæ•°æ®...")
    instruments = D.instruments("csi300")
    fields = ["$close", "$open", "$high", "$low", "$volume"]
    
    df = D.features(
        instruments,
        fields,
        start_time="2022-01-01",
        end_time="2023-12-31",
        freq="day",
    )
    df.columns = ['close', 'open', 'high', 'low', 'volume']
    
    # è®¡ç®—ç›®æ ‡æ”¶ç›Š
    target = df['close'].groupby(level=0).pct_change(5).shift(-5)
    logger.info(f"  æ•°æ®é‡: {len(df):,} è¡Œ")
    
    # éªŒè¯å› å­
    from alpha_agent.core.sandbox import Sandbox
    from alpha_agent.core.evaluator import FactorEvaluator
    
    sandbox = Sandbox(timeout_seconds=10)
    evaluator = FactorEvaluator()
    
    results = []
    factors_to_test = factors[:limit] if limit else factors
    
    logger.info(f"å¼€å§‹éªŒè¯ {len(factors_to_test)} ä¸ªå› å­...")
    
    for i, f in enumerate(factors_to_test):
        factor_id = f.get('id', f'factor_{i}')
        code = f.get('code', '')
        
        if not code or 'def compute_alpha' not in code:
            continue
        
        try:
            # æ‰§è¡Œå› å­
            factor_values, error = sandbox.execute(code, df)
            
            if error or factor_values is None:
                results.append({
                    'id': factor_id,
                    'name': f.get('name', ''),
                    'category': f.get('category', ''),
                    'status': 'error',
                    'ic': None,
                    'icir': None,
                    'error': str(error)[:100] if error else 'empty result',
                })
                continue
            
            # è®¡ç®—IC
            aligned = pd.concat([factor_values, target], axis=1)
            aligned.columns = ['factor', 'return']
            aligned = aligned.dropna()
            
            if len(aligned) < 100:
                results.append({
                    'id': factor_id,
                    'name': f.get('name', ''),
                    'category': f.get('category', ''),
                    'status': 'insufficient_data',
                    'ic': None,
                    'icir': None,
                })
                continue
            
            eval_result = evaluator.evaluate(aligned['factor'], aligned['return'])
            
            results.append({
                'id': factor_id,
                'name': f.get('name', ''),
                'category': f.get('category', ''),
                'status': eval_result.status.value,
                'ic': eval_result.ic,
                'icir': eval_result.icir,
                'rank_ic': eval_result.rank_ic,
            })
            
            if (i + 1) % 20 == 0:
                logger.info(f"  è¿›åº¦: {i+1}/{len(factors_to_test)}")
                
        except Exception as e:
            results.append({
                'id': factor_id,
                'name': f.get('name', ''),
                'category': f.get('category', ''),
                'status': 'exception',
                'ic': None,
                'icir': None,
                'error': str(e)[:100],
            })
    
    # ç”ŸæˆæŠ¥å‘Š
    results_df = pd.DataFrame(results)
    
    logger.info("\n" + "="*50)
    logger.info("éªŒè¯ç»“æœæ±‡æ€»")
    logger.info("="*50)
    
    if len(results_df) > 0:
        valid = results_df[results_df['ic'].notna()]
        logger.info(f"  æ€»å› å­æ•°: {len(results_df)}")
        logger.info(f"  æœ‰æ•ˆå› å­: {len(valid)}")
        
        if len(valid) > 0:
            logger.info(f"  å¹³å‡IC: {valid['ic'].mean():.4f}")
            logger.info(f"  IC>2%: {(valid['ic'].abs() > 0.02).sum()}")
            logger.info(f"  IC>3%: {(valid['ic'].abs() > 0.03).sum()}")
            
            # æŒ‰åˆ†ç±»ç»Ÿè®¡
            logger.info("\næŒ‰åˆ†ç±»ç»Ÿè®¡:")
            for cat in valid['category'].unique():
                cat_df = valid[valid['category'] == cat]
                logger.info(f"  {cat}: {len(cat_df)}ä¸ª, å¹³å‡IC={cat_df['ic'].mean():.4f}")
            
            # Topå› å­
            logger.info("\nTop 10 å› å­:")
            top10 = valid.nlargest(10, 'ic')
            for _, row in top10.iterrows():
                logger.info(f"  {row['id']}: IC={row['ic']:.4f}, ICIR={row['icir']:.2f}")
    
    # ä¿å­˜ç»“æœ
    output_dir = PROJECT_ROOT / "output" / "validation"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"factor_validation_{timestamp}.csv"
    results_df.to_csv(output_file, index=False)
    logger.info(f"\nğŸ“ ç»“æœå·²ä¿å­˜: {output_file}")
    
    return results_df


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def parse_args():
    parser = argparse.ArgumentParser(description="å› å­åº“å¯¼å…¥ä¸éªŒè¯")
    
    parser.add_argument("--import-all", action="store_true", help="å¯¼å…¥æ‰€æœ‰å› å­åˆ°æ•°æ®åº“")
    parser.add_argument("--import-milvus", action="store_true", help="åªå¯¼å…¥åˆ°Milvus")
    parser.add_argument("--import-neo4j", action="store_true", help="åªå¯¼å…¥åˆ°Neo4j")
    parser.add_argument("--import-redis", action="store_true", help="åªå¯¼å…¥åˆ°Redis")
    parser.add_argument("--validate", action="store_true", help="éªŒè¯å› å­IC")
    parser.add_argument("--validate-only", action="store_true", help="åªéªŒè¯ä¸å¯¼å…¥")
    parser.add_argument("--limit", type=int, help="é™åˆ¶å¤„ç†çš„å› å­æ•°é‡")
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    print("\n" + "="*60)
    print("     ğŸ“¦ å› å­åº“å¯¼å…¥ä¸éªŒè¯å·¥å…·")
    print("="*60)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    # åŠ è½½å› å­
    factors = load_all_factors()
    
    if args.limit:
        factors = factors[:args.limit]
        logger.info(f"é™åˆ¶å¤„ç†å‰ {args.limit} ä¸ªå› å­")
    
    # å¯¼å…¥åˆ°æ•°æ®åº“
    if args.import_all or args.import_milvus:
        import_to_milvus_simple(factors)
    
    if args.import_all or args.import_neo4j:
        import_to_neo4j(factors)
    
    if args.import_all or args.import_redis:
        import_to_redis(factors)
    
    # éªŒè¯
    if args.validate or args.validate_only:
        validate_factors(factors, limit=args.limit or 50)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ“ä½œï¼Œæ˜¾ç¤ºå¸®åŠ©
    if not any([args.import_all, args.import_milvus, args.import_neo4j, 
                args.import_redis, args.validate, args.validate_only]):
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python scripts/import_factors.py --import-all        # å¯¼å…¥æ‰€æœ‰æ•°æ®åº“")
        print("  python scripts/import_factors.py --validate-only     # åªéªŒè¯IC")
        print("  python scripts/import_factors.py --import-all --validate  # å¯¼å…¥å¹¶éªŒè¯")
        print("  python scripts/import_factors.py --limit 10          # é™åˆ¶æ•°é‡")
    
    print("\n" + "="*60)
    print("âœ… å®Œæˆ")
    print("="*60)


if __name__ == "__main__":
    main()
