#!/usr/bin/env python
"""
å› å­ç­›é€‰ä¸å›æµ‹å®Œæ•´Pipeline

åŠŸèƒ½:
1. ä»Milvuså‘é‡æ•°æ®åº“/å› å­åº“æå–å€™é€‰å› å­
2. æ‰§è¡Œåˆ†å±‚ç­›é€‰ (å¿«é€ŸIC â†’ å»é‡ â†’ èšç±» â†’ æ­£äº¤åŒ–)
3. å°è£…å› å­ä¾›æ¨¡å‹ä½¿ç”¨
4. è®­ç»ƒé¢„æµ‹æ¨¡å‹å¹¶å›æµ‹
5. è¾“å‡ºå›æµ‹æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    # å®Œæ•´Pipeline (æå– â†’ ç­›é€‰ â†’ å›æµ‹)
    python run_factor_selection.py --mode full
    
    # ä»å› å­åº“åŠ è½½å› å­
    python run_factor_selection.py --mode full --source library --library-sets alpha158
    
    # ä»…ç­›é€‰
    python run_factor_selection.py --mode select --source milvus
    
    # ä»…å›æµ‹ (ä½¿ç”¨å·²ç­›é€‰å› å­)
    python run_factor_selection.py --mode backtest --input output/selection/selected_factors.json
    
    # è‡ªå®šä¹‰å‚æ•°
    python run_factor_selection.py --max-factors 30 --corr-threshold 0.7 --model lgb

APIä½¿ç”¨:
    >>> from alpha_agent.run_factor_selection import quick_start
    >>> result = quick_start(factor_sets=['alpha158'], max_factors=20)
"""
from __future__ import annotations

# æŠ‘åˆ¶Gymå¼ƒç”¨è­¦å‘Šï¼ˆgymç›´æ¥printåˆ°stderrï¼Œéœ€è¦ä¸´æ—¶é‡å®šå‘ï¼‰
import sys as _sys
import io as _io
_original_stderr = _sys.stderr
_sys.stderr = _io.StringIO()
try:
    import gym  # è§¦å‘è­¦å‘Šä½†è¢«æ•è·
except ImportError:
    pass
finally:
    _sys.stderr = _original_stderr
del _sys, _io, _original_stderr

__all__ = [
    # é…ç½®ç±»
    'PipelineConfig',
    'PipelineMode',
    'FactorSource',
    'BacktestResult',
    'ComparisonResult',
    # æ ¸å¿ƒå‡½æ•°
    'run_pipeline',
    'run_full_pipeline',
    'run_selection',
    'run_backtest',
    # ä¾¿æ·API
    'quick_start',
    'run_from_json',
    'compare_factor_sets',  # å› å­é›†å¯¹æ¯”
    # æ•°æ®åŠ è½½
    'load_qlib_data',
    'load_factors_from_milvus',
    'load_factors_from_json',
    'load_factors_from_library',
    # å› å­æ¸…æ´—
    'clean_factor_code',
    'clean_factors',
    # Qlibé›†æˆ
    'run_qlib_benchmark',
    'run_qlib_with_custom_factors',
    'generate_qlib_config',
]

import argparse
import json
import logging
import os
import sys
import time
import warnings
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Callable, Union
from dataclasses import dataclass, field, asdict

import numpy as np
import pandas as pd

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')


# ============================================================
# é…ç½®ç±»å®šä¹‰
# ============================================================

class FactorSource(str, Enum):
    """å› å­æ¥æºæšä¸¾"""
    MILVUS = "milvus"
    FILE = "file"
    LIBRARY = "library"  # ä»alpha_agent.factorsåŠ è½½


class PipelineMode(str, Enum):
    """Pipelineè¿è¡Œæ¨¡å¼"""
    FULL = "full"
    SELECT = "select"
    SELECT_MILVUS = "select-milvus"  # Milvuså› å­ç­›é€‰æ¨¡å¼
    BACKTEST = "backtest"
    COMPARE = "compare"  # å› å­é›†å¯¹æ¯”æ¨¡å¼
    QLIB_BENCHMARK = "qlib-benchmark"
    QLIB_CUSTOM = "qlib-custom"


@dataclass
class PipelineConfig:
    """
    å› å­ç­›é€‰ä¸å›æµ‹Pipelineé…ç½®
    
    é›†ä¸­ç®¡ç†æ‰€æœ‰å‚æ•°ï¼Œæ”¯æŒä»å‘½ä»¤è¡Œå‚æ•°ã€é…ç½®æ–‡ä»¶æˆ–ä»£ç æ„å»º
    """
    # è¿è¡Œæ¨¡å¼
    mode: PipelineMode = PipelineMode.FULL
    
    # æ•°æ®æ¥æº
    source: FactorSource = FactorSource.MILVUS
    input_file: Optional[str] = None
    library_sets: List[str] = field(default_factory=lambda: ["alpha158"])  # libraryæ¨¡å¼çš„å› å­é›†
    
    # ç­›é€‰å‚æ•°
    max_factors: int = 30
    quick_ic_threshold: float = 0.005
    corr_threshold: float = 0.7
    enable_cluster: bool = True
    n_clusters: int = 10
    
    # æ•°æ®å‚æ•°
    instruments: str = "csi300"
    return_days: int = 5
    
    # å›æµ‹æ—¶é—´æ®µ
    train_start: str = "2022-01-01"
    train_end: str = "2022-12-31"
    test_start: str = "2023-01-01"
    test_end: str = "2023-12-31"
    
    # æ¨¡å‹å‚æ•°
    model_type: str = "lgb"
    qlib_models: List[str] = field(default_factory=lambda: ["lgb", "lgb_light", "xgb", "linear"])
    
    # è¾“å‡º
    output_dir: str = "output/selection"
    save_intermediate: bool = True
    
    # Milvusé…ç½®
    milvus_host: str = "localhost"
    milvus_port: int = 19530
    milvus_collection: Optional[str] = None
    milvus_min_ic: Optional[float] = None
    
    # å¯¹æ¯”æ¨¡å¼é…ç½®
    compare_sets: List[str] = field(default_factory=lambda: ["alpha158", "worldquant101", "gtja191"])
    max_factors_per_set: int = 50
    
    def validate(self) -> List[str]:
        """éªŒè¯é…ç½®ï¼Œè¿”å›é”™è¯¯åˆ—è¡¨"""
        errors = []
        
        if self.mode in [PipelineMode.BACKTEST, PipelineMode.QLIB_CUSTOM] and not self.input_file:
            errors.append(f"{self.mode.value}æ¨¡å¼éœ€è¦æŒ‡å®šinput_file")
        
        if self.source == FactorSource.FILE and not self.input_file:
            errors.append("source=fileæ—¶éœ€è¦æŒ‡å®šinput_file")
        
        if self.max_factors < 1:
            errors.append("max_factorså¿…é¡»å¤§äº0")
        
        if not (0 < self.corr_threshold <= 1):
            errors.append("corr_thresholdå¿…é¡»åœ¨(0, 1]ä¹‹é—´")
        
        return errors
    
    @classmethod
    def from_args(cls, args: argparse.Namespace) -> "PipelineConfig":
        """ä»å‘½ä»¤è¡Œå‚æ•°æ„å»ºé…ç½®"""
        qlib_models = [m.strip() for m in args.qlib_models.split(',')] if hasattr(args, 'qlib_models') else ["lgb"]
        library_sets = [s.strip() for s in args.library_sets.split(',')] if hasattr(args, 'library_sets') else ["alpha158"]
        compare_sets = [s.strip() for s in args.compare_sets.split(',')] if hasattr(args, 'compare_sets') else ["alpha158", "worldquant101", "gtja191"]
        max_factors_per_set = getattr(args, 'max_factors_per_set', 50)
        
        return cls(
            mode=PipelineMode(args.mode),
            source=FactorSource(args.source),
            input_file=args.input,
            library_sets=library_sets,
            max_factors=args.max_factors,
            quick_ic_threshold=args.quick_ic,
            corr_threshold=args.corr_threshold,
            instruments=args.instruments,
            train_start=args.train_start,
            train_end=args.train_end,
            test_start=args.test_start,
            test_end=args.test_end,
            model_type=args.model,
            qlib_models=qlib_models,
            output_dir=args.output,
            compare_sets=compare_sets,
            max_factors_per_set=max_factors_per_set,
        )

# è®¾ç½®é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT.parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================
# æ•°æ®åŠ è½½
# ============================================================

def load_qlib_data(
    instruments: str = "csi300",
    start_date: str = "2022-01-01",
    end_date: str = "2023-12-31",
    return_days: int = 5,
) -> tuple:
    """
    åŠ è½½Qlibæ•°æ®
    
    å‚è€ƒ: https://www.wuzao.com/p/qlib/document/start/getdata.html
    
    Args:
        instruments: è‚¡ç¥¨æ±  (csi300, csi500, all)
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        return_days: æ”¶ç›Šè®¡ç®—å¤©æ•°
    
    Returns:
        (data, target) - DataFrameå’Œç›®æ ‡æ”¶ç›Š
    """
    try:
        import qlib
        from qlib.data import D
        from qlib.config import REG_CN
        
        # Qlibæ•°æ®è·¯å¾„
        provider_uri = os.path.expanduser("~/.qlib/qlib_data/cn_data")
        if not os.path.exists(provider_uri):
            logger.warning(f"Qlibæ•°æ®ä¸å­˜åœ¨: {provider_uri}")
            logger.info("è¯·è¿è¡Œ: python -m qlib.run.data_collector qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn")
            return None, None
        
        # åˆå§‹åŒ–Qlib
        qlib.init(provider_uri=provider_uri, region=REG_CN)
        logger.info(f"Qlibåˆå§‹åŒ–æˆåŠŸ: {provider_uri}")
        
        # è·å–è‚¡ç¥¨æ± 
        instruments_list = D.instruments(instruments)
        logger.info(f"è‚¡ç¥¨æ± : {instruments}")
        
        # å®šä¹‰å­—æ®µ - ä½¿ç”¨Qlibè¡¨è¾¾å¼è¯­æ³•
        fields = [
            "$close",    # æ”¶ç›˜ä»·
            "$open",     # å¼€ç›˜ä»·
            "$high",     # æœ€é«˜ä»·
            "$low",      # æœ€ä½ä»·
            "$volume",   # æˆäº¤é‡
            "$vwap",     # æˆäº¤å‡ä»·
            "$turn",     # æ¢æ‰‹ç‡
            "$factor",   # å¤æƒå› å­
        ]
        
        # åŠ è½½æ•°æ®
        logger.info(f"åŠ è½½æ•°æ®: {start_date} ~ {end_date}")
        df = D.features(
            instruments_list,
            fields,
            start_time=start_date,
            end_time=end_date,
            freq="day",
        )
        
        # é‡å‘½ååˆ—
        df.columns = ['close', 'open', 'high', 'low', 'volume', 'vwap', 'turn', 'adj_factor']
        
        # æ·»åŠ æ´¾ç”ŸæŒ‡æ ‡ä»¥æ”¯æŒæ›´å¤šå› å­
        # æ—¥æ”¶ç›Šç‡
        df['returns'] = df.groupby(level='instrument')['close'].pct_change()
        
        # å¸‚å€¼ä¼°ç®— (ç”¨ä»·æ ¼*æˆäº¤é‡*100è¿‘ä¼¼ï¼Œå•ä½ï¼šå…ƒ)
        df['market_cap'] = df['close'] * df['volume'] * 100
        
        # å¸‚åœºæ”¶ç›Š (æ‰€æœ‰è‚¡ç¥¨å¹³å‡æ”¶ç›Š)
        df['market_ret'] = df.groupby(level='datetime')['returns'].transform('mean')
        
        # æ¢æ‰‹ç‡
        if 'turn' in df.columns:
            df['turnover'] = df['turn']
        
        # æˆäº¤é¢
        df['amount'] = df['close'] * df['volume']
        
        # æŒ¯å¹…
        df['amplitude'] = (df['high'] - df['low']) / df['close'].shift(1)
        
        # è®¡ç®—æœªæ¥æ”¶ç›Šä½œä¸ºé¢„æµ‹ç›®æ ‡
        # Ref($close, -N) = Nå¤©åçš„æ”¶ç›˜ä»·
        target = df['close'].groupby(level='instrument').pct_change(return_days).shift(-return_days)
        
        # ç»Ÿè®¡
        n_stocks = df.index.get_level_values('instrument').nunique()
        n_days = df.index.get_level_values('datetime').nunique()
        
        logger.info(f"Qlibæ•°æ®åŠ è½½å®Œæˆ:")
        logger.info(f"  - è‚¡ç¥¨æ•°: {n_stocks}")
        logger.info(f"  - äº¤æ˜“æ—¥: {n_days}")
        logger.info(f"  - æ€»è®°å½•: {len(df):,}")
        
        return df, target
        
    except ImportError:
        logger.error("Qlibæœªå®‰è£…: pip install pyqlib")
        return None, None
    except Exception as e:
        logger.warning(f"Qlibæ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None


# ä»selectionæ¨¡å—å¯¼å…¥å› å­æ¸…æ´—ã€ç­›é€‰å’Œæ•°æ®é¢„å¤„ç†åŠŸèƒ½
from alpha_agent.selection import (
    clean_factors,
    clean_factor_code,
    FactorCleaner,
    FactorSelector,
    SelectionResult,
    FactorWrapper,
    # æ•°æ®é¢„å¤„ç†
    add_derived_fields,
    prepare_train_test_data,
)

# ä¿ç•™æ—§å‡½æ•°åä»¥å…¼å®¹
def filter_factors_by_available_columns(factors: List[Dict], available_columns: List[str]) -> List[Dict]:
    """å…¼å®¹æ¥å£ï¼šæ”¹ä¸ºè°ƒç”¨clean_factorsï¼Œä¸å†è¿‡æ»¤å› å­"""
    return clean_factors(factors, available_columns)


def load_factors_from_milvus(
    host: str = "localhost",
    port: int = 19530,
    collection_name: str = None,
    min_ic: float = None,
) -> List[Dict]:
    """
    ä»Milvuså‘é‡æ•°æ®åº“åŠ è½½å› å­
    
    Args:
        host: Milvusä¸»æœº
        port: Milvusç«¯å£
        collection_name: é›†åˆåç§°
        min_ic: æœ€å°ICè¿‡æ»¤
    
    Returns:
        å› å­åˆ—è¡¨
    """
    logger.info(f"ä»MilvusåŠ è½½å› å­: {host}:{port}")
    
    try:
        from alpha_agent.memory.vector_store import MilvusStore
        from alpha_agent.config import vector_db_config
        
        store = MilvusStore(
            host=host,
            port=port,
            collection_name=collection_name or vector_db_config.collection_name,
        )
        
        if not store.connect():
            logger.error("Milvusè¿æ¥å¤±è´¥")
            return []
        
        store.create_collection()
        
        # è·å–å› å­æ•°é‡
        count = store.count()
        logger.info(f"Milvusä¸­å…±æœ‰ {count} ä¸ªå› å­")
        
        # è·å–æ‰€æœ‰å› å­
        factors = store.get_all_factors(limit=10000, min_ic=min_ic)
        
        store.disconnect()
        
        logger.info(f"ä»MilvusåŠ è½½ {len(factors)} ä¸ªå› å­")
        return factors
        
    except ImportError as e:
        logger.error(f"Milvusä¾èµ–æœªå®‰è£…: {e}")
        logger.error("è¯·å®‰è£…: pip install pymilvus")
        return []
    except Exception as e:
        logger.error(f"ä»MilvusåŠ è½½å¤±è´¥: {e}")
        return []


def load_factors_from_json(path: str) -> List[Dict]:
    """ä»JSONæ–‡ä»¶åŠ è½½å› å­"""
    logger.info(f"ä»æ–‡ä»¶åŠ è½½å› å­: {path}")
    
    if not os.path.exists(path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return []
    
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        logger.error(f"JSONè§£æå¤±è´¥: {e}")
        return []
    
    if isinstance(data, list):
        factors = data
    elif isinstance(data, dict) and 'factors' in data:
        # æ”¯æŒ{'factors': [...]} æˆ– {'factors': {'name': {...}, ...}}
        factors_data = data['factors']
        if isinstance(factors_data, list):
            factors = factors_data
        else:
            factors = list(factors_data.values())
    else:
        factors = [data]
    
    logger.info(f"ä»æ–‡ä»¶åŠ è½½ {len(factors)} ä¸ªå› å­")
    return factors


def load_factors_from_library(
    factor_sets: List[str] = None,
    max_factors: int = None,
) -> List[Dict]:
    """
    ä»alpha_agent.factorså› å­åº“åŠ è½½å› å­
    
    Args:
        factor_sets: å› å­é›†åˆ—è¡¨ï¼Œå¯é€‰: alpha158, alpha360, worldquant101, gtja191, classic
        max_factors: æ¯ä¸ªå› å­é›†æœ€å¤§åŠ è½½æ•°é‡
    
    Returns:
        å› å­åˆ—è¡¨
    """
    logger.info(f"ä»å› å­åº“åŠ è½½å› å­: {factor_sets}")
    
    if factor_sets is None:
        factor_sets = ["alpha158"]
    
    factors = []
    
    try:
        from alpha_agent.factors import (
            ALPHA158_FACTORS,
            ALPHA360_FACTORS,
            WORLDQUANT_101_FACTORS,
            GTJA191_FACTORS,
            ALL_CLASSIC_FACTORS,
        )
        
        factor_map = {
            "alpha158": ALPHA158_FACTORS,
            "alpha360": ALPHA360_FACTORS,
            "worldquant101": WORLDQUANT_101_FACTORS,
            "gtja191": GTJA191_FACTORS,
            "classic": ALL_CLASSIC_FACTORS,
        }
        
        for factor_set in factor_sets:
            set_name = factor_set.lower()
            if set_name not in factor_map:
                logger.warning(f"æœªçŸ¥å› å­é›†: {factor_set}ï¼Œæ”¯æŒ: {list(factor_map.keys())}")
                continue
            
            set_factors = factor_map[set_name]
            
            # ç»Ÿä¸€æ ¼å¼ - æ”¯æŒClassicFactor dataclasså’Œå­—å…¸æ ¼å¼
            for i, f in enumerate(set_factors):
                if max_factors and i >= max_factors:
                    break
                
                # åˆ¤æ–­æ˜¯dataclassè¿˜æ˜¯å­—å…¸
                if hasattr(f, 'id'):
                    # ClassicFactor dataclass
                    factor_dict = {
                        'id': getattr(f, 'id', f"{set_name}_{i:03d}"),
                        'name': getattr(f, 'name', getattr(f, 'id', f"{set_name}_{i:03d}")),
                        'code': getattr(f, 'code', ''),
                        'expression': getattr(f, 'code', ''),  # ClassicFactorç”¨codeå­˜è¡¨è¾¾å¼
                        'description': getattr(f, 'description', ''),
                        'category': str(getattr(f, 'category', set_name)),
                        'source': f"library:{set_name}",
                    }
                else:
                    # å­—å…¸æ ¼å¼
                    factor_dict = {
                        'id': f.get('id', f"{set_name}_{i:03d}"),
                        'name': f.get('name', f.get('id', f"{set_name}_{i:03d}")),
                        'code': f.get('code', f.get('expression', '')),
                        'expression': f.get('expression', f.get('code', '')),
                        'description': f.get('description', ''),
                        'category': f.get('category', set_name),
                        'source': f"library:{set_name}",
                    }
                factors.append(factor_dict)
            
            logger.info(f"  - {set_name}: {min(len(set_factors), max_factors or len(set_factors))} ä¸ªå› å­")
        
        logger.info(f"ä»å› å­åº“å…±åŠ è½½ {len(factors)} ä¸ªå› å­")
        return factors
        
    except ImportError as e:
        logger.error(f"å› å­åº“å¯¼å…¥å¤±è´¥: {e}")
        return []
    except Exception as e:
        logger.error(f"ä»å› å­åº“åŠ è½½å¤±è´¥: {e}")
        return []


# ============================================================
# å› å­æ‰§è¡Œå™¨ (å¤ç”¨ç°æœ‰Sandbox)
# ============================================================

def create_sandbox_executor(data: pd.DataFrame):
    """åˆ›å»ºæ²™ç®±æ‰§è¡Œå™¨ - ä½¿ç”¨ç°æœ‰Sandbox"""
    from alpha_agent.core.sandbox import execute_code
    
    _error_count = [0]
    
    def executor(code: str, df: pd.DataFrame = None) -> Optional[pd.Series]:
        if df is None:
            df = data
        
        result, error = execute_code(code, df, timeout_seconds=30)
        if error:
            _error_count[0] += 1
            if _error_count[0] <= 3:
                # åªæ‰“å°ç®€çŸ­é”™è¯¯
                short_error = error.split('\n')[0] if '\n' in error else error
                logger.warning(f"æ‰§è¡Œå¤±è´¥: {short_error}")
            return None
        return result
    
    return executor


# ============================================================
# å›æµ‹ç»“æœæ•°æ®ç±»
# ============================================================

@dataclass
class BacktestResult:
    """å›æµ‹ç»“æœ"""
    # æ”¶ç›ŠæŒ‡æ ‡
    total_return: float = 0.0
    annual_return: float = 0.0
    sharpe_ratio: float = 0.0
    max_drawdown: float = 0.0
    
    # æ¨¡å‹æŒ‡æ ‡
    ic_mean: float = 0.0
    icir: float = 0.0
    
    # åˆ†ç»„æ”¶ç›Š
    top_group_return: float = 0.0
    bottom_group_return: float = 0.0
    long_short_return: float = 0.0
    
    # å…¶ä»–
    factor_count: int = 0
    model_type: str = ""
    train_period: str = ""
    test_period: str = ""
    
    def to_dict(self) -> Dict:
        """è½¬ä¸ºå­—å…¸ï¼Œç¡®ä¿JSONå¯åºåˆ—åŒ–"""
        d = asdict(self)
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        for k, v in d.items():
            if hasattr(v, 'item'):  # numpy scalar
                d[k] = v.item()
            elif isinstance(v, float) and not isinstance(v, (int, bool)):
                d[k] = float(v)
        return d
    
    def summary(self) -> str:
        return f"""
============================================================
                    ğŸ“ˆ å›æµ‹ç»“æœ
============================================================
æ¨¡å‹: {self.model_type}
å› å­æ•°: {self.factor_count}
è®­ç»ƒæœŸ: {self.train_period}
æµ‹è¯•æœŸ: {self.test_period}

æ”¶ç›ŠæŒ‡æ ‡:
  - å¹´åŒ–æ”¶ç›Š: {self.annual_return*100:.2f}%
  - å¤æ™®æ¯”ç‡: {self.sharpe_ratio:.2f}
  - æœ€å¤§å›æ’¤: {self.max_drawdown*100:.2f}%

æ¨¡å‹æŒ‡æ ‡:
  - ICå‡å€¼: {self.ic_mean:.4f}
  - ICIR: {self.icir:.2f}

åˆ†ç»„æ”¶ç›Š (å¹´åŒ–):
  - Topç»„: {self.top_group_return*100:.2f}%
  - Bottomç»„: {self.bottom_group_return*100:.2f}%
  - å¤šç©ºæ”¶ç›Š: {self.long_short_return*100:.2f}%
============================================================
"""


# ============================================================
# æ¨¡å‹è®­ç»ƒä¸å›æµ‹
# ============================================================

def train_model(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    model_type: str = "lgb",
) -> Any:
    """
    è®­ç»ƒé¢„æµ‹æ¨¡å‹
    
    Args:
        X_train: ç‰¹å¾
        y_train: æ ‡ç­¾
        model_type: æ¨¡å‹ç±»å‹ (lgb, linear, ridge)
    
    Returns:
        è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    if model_type == "lgb":
        try:
            from lightgbm import LGBMRegressor
            model = LGBMRegressor(
                n_estimators=200,
                max_depth=6,
                learning_rate=0.05,
                num_leaves=31,
                min_child_samples=20,
                reg_alpha=0.1,
                reg_lambda=0.1,
                n_jobs=-1,
                verbose=-1,
                random_state=42,
            )
        except ImportError:
            logger.warning("LightGBMæœªå®‰è£…ï¼Œä½¿ç”¨Ridgeå›å½’")
            model_type = "ridge"
    
    if model_type == "linear":
        from sklearn.linear_model import LinearRegression
        model = LinearRegression()
    
    if model_type == "ridge":
        from sklearn.linear_model import Ridge
        model = Ridge(alpha=1.0)
    
    # è®­ç»ƒ
    valid_idx = ~(y_train.isna() | X_train.isna().any(axis=1))
    X_clean = X_train.loc[valid_idx]
    y_clean = y_train.loc[valid_idx]
    
    logger.info(f"è®­ç»ƒæ¨¡å‹: {model_type}, æ ·æœ¬æ•°: {len(X_clean):,}")
    model.fit(X_clean, y_clean)
    
    return model


def run_backtest(
    wrapper,
    data: pd.DataFrame,
    target: pd.Series,
    train_start: str = "2022-01-01",
    train_end: str = "2022-12-31",
    test_start: str = "2023-01-01",
    test_end: str = "2023-12-31",
    model_type: str = "lgb",
    n_groups: int = 5,
) -> BacktestResult:
    """
    æ‰§è¡Œå›æµ‹
    
    Args:
        wrapper: FactorWrapperå®ä¾‹
        data: åŸå§‹æ•°æ®
        target: ç›®æ ‡æ”¶ç›Š
        train_start: è®­ç»ƒå¼€å§‹æ—¥æœŸ
        train_end: è®­ç»ƒç»“æŸæ—¥æœŸ
        test_start: æµ‹è¯•å¼€å§‹æ—¥æœŸ
        test_end: æµ‹è¯•ç»“æŸæ—¥æœŸ
        model_type: æ¨¡å‹ç±»å‹
        n_groups: åˆ†ç»„æ•°
    
    Returns:
        BacktestResult
    """
    logger.info("="*60)
    logger.info("     ğŸ“Š å¼€å§‹å›æµ‹")
    logger.info("="*60)
    
    # 1. è®¡ç®—å› å­å€¼
    logger.info("\nğŸ“Š Step 1: è®¡ç®—å› å­å€¼")
    factor_df = wrapper.compute(data, n_workers=4)
    logger.info(f"è®¡ç®—å®Œæˆ: {factor_df.shape[1]} ä¸ªå› å­")
    
    if factor_df.empty or factor_df.shape[1] == 0:
        logger.error("æ— æœ‰æ•ˆå› å­å€¼")
        return BacktestResult()
    
    # 2. å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨æ•°æ®é¢„å¤„ç†æ¨¡å—ï¼‰
    logger.info("\nğŸ“Š Step 2: å‡†å¤‡è®­ç»ƒ/æµ‹è¯•æ•°æ®")
    
    X_train, y_train, X_test, y_test = prepare_train_test_data(
        factor_df=factor_df,
        target=target,
        train_start=train_start,
        train_end=train_end,
        test_start=test_start,
        test_end=test_end,
        fill_strategy='fill_zero',
    )
    
    if len(X_train) == 0 or len(X_test) == 0:
        logger.error("è®­ç»ƒé›†æˆ–æµ‹è¯•é›†ä¸ºç©º")
        return BacktestResult()
    
    # 3. è®­ç»ƒæ¨¡å‹
    logger.info("\nğŸ“Š Step 3: è®­ç»ƒæ¨¡å‹")
    model = train_model(X_train, y_train, model_type)
    
    # 4. é¢„æµ‹
    logger.info("\nğŸ“Š Step 4: ç”Ÿæˆé¢„æµ‹")
    y_pred = model.predict(X_test.fillna(0))
    pred_series = pd.Series(y_pred, index=X_test.index, name='prediction')
    
    # 5. è®¡ç®—IC
    logger.info("\nğŸ“Š Step 5: è®¡ç®—IC")
    ic_series = _compute_daily_ic(pred_series, y_test)
    ic_mean = ic_series.mean()
    icir = ic_mean / (ic_series.std() + 1e-8)
    logger.info(f"ICå‡å€¼: {ic_mean:.4f}, ICIR: {icir:.2f}")
    
    # 6. åˆ†ç»„å›æµ‹
    logger.info("\nğŸ“Š Step 6: åˆ†ç»„å›æµ‹")
    group_returns = _compute_group_returns(pred_series, y_test, n_groups)
    
    top_return = group_returns.get(f'group_{n_groups}', 0)
    bottom_return = group_returns.get('group_1', 0)
    long_short = top_return - bottom_return
    
    logger.info(f"Topç»„å¹´åŒ–: {top_return*100:.2f}%")
    logger.info(f"Bottomç»„å¹´åŒ–: {bottom_return*100:.2f}%")
    logger.info(f"å¤šç©ºæ”¶ç›Šå¹´åŒ–: {long_short*100:.2f}%")
    
    # 7. è®¡ç®—ç»„åˆæ”¶ç›Š
    logger.info("\nğŸ“Š Step 7: è®¡ç®—ç»„åˆæ”¶ç›Š")
    portfolio_return, sharpe, max_dd = _compute_portfolio_metrics(pred_series, y_test)
    
    result = BacktestResult(
        total_return=portfolio_return,
        annual_return=portfolio_return * 252 / len(ic_series) if len(ic_series) > 0 else 0,
        sharpe_ratio=sharpe,
        max_drawdown=max_dd,
        ic_mean=ic_mean,
        icir=icir,
        top_group_return=top_return,
        bottom_group_return=bottom_return,
        long_short_return=long_short,
        factor_count=len(X_train.columns),
        model_type=model_type,
        train_period=f"{train_start} ~ {train_end}",
        test_period=f"{test_start} ~ {test_end}",
    )
    
    logger.info(result.summary())
    
    return result


def _compute_daily_ic(pred: pd.Series, actual: pd.Series) -> pd.Series:
    """è®¡ç®—æ¯æ—¥æˆªé¢IC"""
    df = pd.concat([pred, actual], axis=1)
    df.columns = ['pred', 'actual']
    
    if hasattr(df.index, 'get_level_values'):
        # MultiIndex
        dates = df.index.get_level_values('datetime').unique()
        daily_ic = []
        for date in dates:
            try:
                day_data = df.xs(date, level='datetime')
                if len(day_data) > 10:
                    ic = day_data['pred'].corr(day_data['actual'], method='spearman')
                    if not np.isnan(ic):
                        daily_ic.append(ic)
            except Exception:
                continue
        return pd.Series(daily_ic)
    else:
        return pd.Series([pred.corr(actual, method='spearman')])


def _compute_group_returns(
    pred: pd.Series,
    actual: pd.Series,
    n_groups: int = 5,
) -> Dict[str, float]:
    """è®¡ç®—åˆ†ç»„æ”¶ç›Š"""
    df = pd.concat([pred, actual], axis=1)
    df.columns = ['pred', 'actual']
    
    group_returns = {f'group_{i+1}': [] for i in range(n_groups)}
    
    if hasattr(df.index, 'get_level_values'):
        dates = df.index.get_level_values('datetime').unique()
        
        for date in dates:
            try:
                day_data = df.xs(date, level='datetime').dropna()
                if len(day_data) < n_groups * 5:
                    continue
                
                # æŒ‰é¢„æµ‹å€¼åˆ†ç»„
                day_data['group'] = pd.qcut(day_data['pred'], n_groups, labels=False, duplicates='drop')
                
                for g in range(n_groups):
                    g_return = day_data[day_data['group'] == g]['actual'].mean()
                    if not np.isnan(g_return):
                        group_returns[f'group_{g+1}'].append(g_return)
            except Exception:
                continue
    
    # è®¡ç®—å¹´åŒ–æ”¶ç›Š
    result = {}
    for g, returns in group_returns.items():
        if returns:
            mean_daily = np.mean(returns)
            result[g] = mean_daily * 252  # å¹´åŒ–
        else:
            result[g] = 0
    
    return result


def _compute_portfolio_metrics(
    pred: pd.Series,
    actual: pd.Series,
    top_ratio: float = 0.2,
) -> Tuple[float, float, float]:
    """è®¡ç®—ç»„åˆæŒ‡æ ‡"""
    df = pd.concat([pred, actual], axis=1)
    df.columns = ['pred', 'actual']
    
    daily_returns = []
    
    if hasattr(df.index, 'get_level_values'):
        dates = df.index.get_level_values('datetime').unique()
        
        for date in dates:
            try:
                day_data = df.xs(date, level='datetime').dropna()
                if len(day_data) < 10:
                    continue
                
                # é€‰æ‹©é¢„æµ‹å€¼æœ€é«˜çš„top_ratio
                threshold = day_data['pred'].quantile(1 - top_ratio)
                selected = day_data[day_data['pred'] >= threshold]
                
                if len(selected) > 0:
                    daily_ret = selected['actual'].mean()
                    daily_returns.append(daily_ret)
            except Exception:
                continue
    
    if not daily_returns:
        return 0, 0, 0
    
    returns = pd.Series(daily_returns)
    
    # æ€»æ”¶ç›Š
    total_return = (1 + returns).prod() - 1
    
    # å¤æ™®æ¯”ç‡ (å‡è®¾æ— é£é™©åˆ©ç‡ä¸º0)
    sharpe = returns.mean() / (returns.std() + 1e-8) * np.sqrt(252)
    
    # æœ€å¤§å›æ’¤
    cumulative = (1 + returns).cumprod()
    peak = cumulative.expanding().max()
    drawdown = (cumulative - peak) / peak
    max_dd = abs(drawdown.min())
    
    return total_return, sharpe, max_dd


# ============================================================
# Qlibå¤šæ¨¡å‹å›æµ‹ (ä½¿ç”¨ç°æœ‰çš„QlibBenchmark)
# ============================================================

def run_qlib_benchmark(
    models: List[str] = None,
    instruments: str = "csi300",
    train_period: Tuple[str, str] = ("2018-01-01", "2021-12-31"),
    valid_period: Tuple[str, str] = ("2022-01-01", "2022-06-30"),
    test_period: Tuple[str, str] = ("2022-07-01", "2023-12-31"),
    output_dir: str = "output/selection",
) -> Dict:
    """
    ä½¿ç”¨QlibBenchmarkè¿›è¡Œå¤šæ¨¡å‹å›æµ‹
    
    Args:
        models: æ¨¡å‹åˆ—è¡¨ (é»˜è®¤: lgb, xgb, linear)
        instruments: è‚¡ç¥¨æ± 
        train_period: è®­ç»ƒæœŸ
        valid_period: éªŒè¯æœŸ
        test_period: æµ‹è¯•æœŸ
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        å›æµ‹ç»“æœå­—å…¸
    """
    logger.info("="*60)
    logger.info("     ğŸ”¬ Qlibå¤šæ¨¡å‹åŸºå‡†æµ‹è¯•")
    logger.info("="*60)
    
    try:
        from alpha_agent.modeling.qlib_model_zoo import QlibBenchmark, QlibModelZoo
        
        # é»˜è®¤æ¨¡å‹
        if models is None:
            models = ["lgb", "lgb_light", "xgb", "linear"]
        
        logger.info(f"æ¨¡å‹: {models}")
        logger.info(f"è®­ç»ƒæœŸ: {train_period}")
        logger.info(f"æµ‹è¯•æœŸ: {test_period}")
        
        # åˆ›å»ºåŸºå‡†æµ‹è¯•
        benchmark = QlibBenchmark(models=models)
        
        # è¿è¡Œå›æµ‹
        comparison_df = benchmark.run(
            instruments=instruments,
            train_period=train_period,
            valid_period=valid_period,
            test_period=test_period,
            experiment_name="factor_selection",
        )
        
        # ä¿å­˜ç»“æœ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = output_path / f"qlib_benchmark_{timestamp}.csv"
        comparison_df.to_csv(result_file, index=False)
        logger.info(f"ç»“æœå·²ä¿å­˜: {result_file}")
        
        # ä¸å®˜æ–¹åŸºå‡†å¯¹æ¯”
        QlibModelZoo.compare_with_official(benchmark.results)
        
        return {
            'comparison': comparison_df.to_dict(),
            'results': {k: v.__dict__ for k, v in benchmark.results.items()},
            'file': str(result_file),
        }
        
    except ImportError as e:
        logger.error(f"QlibBenchmarkå¯¼å…¥å¤±è´¥: {e}")
        logger.info("è¯·ç¡®ä¿Qlibå·²å®‰è£…: pip install pyqlib")
        return {}
    except Exception as e:
        logger.error(f"Qlibå›æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {}


def run_qlib_with_custom_factors(
    wrapper,
    models: List[str] = None,
    instruments: str = "csi300",
    train_period: Tuple[str, str] = ("2018-01-01", "2021-12-31"),
    valid_period: Tuple[str, str] = ("2022-01-01", "2022-06-30"),
    test_period: Tuple[str, str] = ("2022-07-01", "2023-12-31"),
    output_dir: str = "output/selection",
) -> Dict:
    """
    ä½¿ç”¨ç­›é€‰çš„å› å­ + Qlibæ¨¡å‹è¿›è¡Œå›æµ‹
    
    Args:
        wrapper: FactorWrapperå®ä¾‹ (åŒ…å«ç­›é€‰çš„å› å­)
        models: Qlibæ¨¡å‹åˆ—è¡¨
        instruments: è‚¡ç¥¨æ± 
        train_period: è®­ç»ƒæœŸ
        valid_period: éªŒè¯æœŸ
        test_period: æµ‹è¯•æœŸ
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        å›æµ‹ç»“æœ
    """
    logger.info("="*60)
    logger.info("     ğŸ§ª è‡ªå®šä¹‰å› å­ + Qlibæ¨¡å‹å›æµ‹")
    logger.info("="*60)
    
    try:
        import qlib
        from qlib.data import D
        from qlib.config import REG_CN
        from qlib.utils import init_instance_by_config
        from qlib.workflow import R
        from qlib.workflow.record_temp import SignalRecord, SigAnaRecord
        
        # åˆå§‹åŒ–Qlib
        provider_uri = os.path.expanduser("~/.qlib/qlib_data/cn_data")
        if not os.path.exists(provider_uri):
            logger.error(f"Qlibæ•°æ®ä¸å­˜åœ¨: {provider_uri}")
            return {}
        
        qlib.init(provider_uri=provider_uri, region=REG_CN)
        logger.info("Qlibå·²åˆå§‹åŒ–")
        
        # è·å–å› å­è¡¨è¾¾å¼
        expressions = wrapper.to_qlib_expressions()
        logger.info(f"å› å­æ•°: {len(expressions)}")
        
        if not expressions:
            logger.error("æ— æœ‰æ•ˆå› å­è¡¨è¾¾å¼")
            return {}
        
        # æ„å»ºè‡ªå®šä¹‰Handleré…ç½®
        fields = [e['expression'] for e in expressions]
        names = [e['name'] for e in expressions]
        
        # åˆ›å»ºæ•°æ®é›†é…ç½® (ä½¿ç”¨è‡ªå®šä¹‰å› å­)
        dataset_config = {
            "class": "DatasetH",
            "module_path": "qlib.data.dataset",
            "kwargs": {
                "handler": {
                    "class": "DataHandlerLP",
                    "module_path": "qlib.data.dataset.handler",
                    "kwargs": {
                        "start_time": train_period[0],
                        "end_time": test_period[1],
                        "fit_start_time": train_period[0],
                        "fit_end_time": train_period[1],
                        "instruments": instruments,
                        "infer_processors": [
                            {"class": "RobustZScoreNorm", "kwargs": {"clip_outlier": True}},
                            {"class": "Fillna", "kwargs": {"fill_value": 0}},
                        ],
                        "learn_processors": [
                            {"class": "DropnaLabel"},
                            {"class": "CSRankNorm"},
                        ],
                        "data_loader": {
                            "class": "QlibDataLoader",
                            "kwargs": {
                                "config": {
                                    "feature": (fields, names),
                                    "label": (["Ref($close, -5) / $close - 1"], ["LABEL0"]),
                                },
                            },
                        },
                    },
                },
                "segments": {
                    "train": train_period,
                    "valid": valid_period,
                    "test": test_period,
                },
            },
        }
        
        # å¯¼å…¥æ¨¡å‹é…ç½®
        from alpha_agent.modeling.qlib_model_zoo import QlibModelZoo
        
        if models is None:
            models = ["lgb", "linear"]
        
        results = {}
        
        for model_name in models:
            logger.info(f"\nè®­ç»ƒæ¨¡å‹: {model_name}")
            
            model_info = QlibModelZoo.get_model_info(model_name)
            if model_info is None:
                continue
            
            display_name, category, model_config = model_info
            
            try:
                # åˆå§‹åŒ–
                model = init_instance_by_config(model_config)
                dataset = init_instance_by_config(dataset_config)
                
                # è®­ç»ƒ
                with R.start(experiment_name=f"custom_factors_{model_name}"):
                    model.fit(dataset)
                    pred = model.predict(dataset)
                    
                    recorder = R.get_recorder()
                    sr = SignalRecord(model, dataset, recorder)
                    sr.generate()
                    
                    # ICåˆ†æ
                    try:
                        sar = SigAnaRecord(recorder)
                        sar.generate()
                        
                        ic_series = recorder.load_object("sig_analysis/ic.pkl")
                        if ic_series is not None:
                            ic = float(ic_series.mean())
                            icir = float(ic_series.mean() / ic_series.std()) if ic_series.std() > 0 else 0
                            logger.info(f"  âœ“ {display_name}: IC={ic:.4f}, ICIR={icir:.2f}")
                            
                            results[model_name] = {
                                'name': display_name,
                                'ic': ic,
                                'icir': icir,
                                'status': 'success',
                            }
                    except Exception as e:
                        logger.warning(f"ICåˆ†æå¤±è´¥: {e}")
                        results[model_name] = {'name': display_name, 'status': 'error', 'error': str(e)}
                        
            except Exception as e:
                logger.error(f"æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                results[model_name] = {'name': display_name, 'status': 'error', 'error': str(e)}
        
        # ä¿å­˜ç»“æœ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = output_path / f"custom_factor_benchmark_{timestamp}.json"
        with open(result_file, 'w') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nç»“æœå·²ä¿å­˜: {result_file}")
        return results
        
    except ImportError as e:
        logger.error(f"Qlibå¯¼å…¥å¤±è´¥: {e}")
        return {}
    except Exception as e:
        logger.error(f"å›æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {}


# ============================================================
# ä¸»æµç¨‹
# ============================================================

def run_full_pipeline(
    source: str = "milvus",
    input_file: str = None,
    max_factors: int = 30,
    quick_ic_threshold: float = 0.005,
    corr_threshold: float = 0.7,
    instruments: str = "csi300",
    model_type: str = "lgb",
    output_dir: str = "output/selection",
    run_backtest_flag: bool = True,
    train_start: str = "2022-01-01",
    train_end: str = "2022-12-31",
    test_start: str = "2023-01-01",
    test_end: str = "2023-12-31",
    library_sets: Optional[List[str]] = None,
) -> Dict:
    """
    è¿è¡Œå®Œæ•´Pipeline: æå– â†’ ç­›é€‰ â†’ å›æµ‹
    
    Args:
        source: å› å­æ¥æº ("milvus", "file", "library")
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ (source="file"æ—¶ä½¿ç”¨)
        max_factors: æœ€å¤§å› å­æ•°
        quick_ic_threshold: å¿«é€ŸICé˜ˆå€¼
        corr_threshold: ç›¸å…³æ€§é˜ˆå€¼
        instruments: è‚¡ç¥¨æ± 
        model_type: æ¨¡å‹ç±»å‹ (lgb, linear, ridge)
        output_dir: è¾“å‡ºç›®å½•
        run_backtest_flag: æ˜¯å¦æ‰§è¡Œå›æµ‹
    """
    logger.info("="*60)
    logger.info("     ğŸš€ å› å­ç­›é€‰ä¸å›æµ‹Pipeline")
    logger.info("="*60)
    logger.info(f"æ¥æº: {source}")
    logger.info(f"æœ€å¤§å› å­æ•°: {max_factors}")
    logger.info(f"æ¨¡å‹: {model_type}")
    logger.info("="*60)
    
    start_time = time.time()
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ========================================
    # Phase 1: åŠ è½½Qlibæ•°æ®
    # ========================================
    logger.info("\n" + "="*60)
    logger.info("  Phase 1: åŠ è½½Qlibæ•°æ®")
    logger.info("="*60)
    
    data, target = load_qlib_data(instruments=instruments)
    if data is None:
        logger.error("Qlibæ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·ç¡®ä¿:")
        logger.error("  1. å·²å®‰è£…Qlib: pip install pyqlib")
        logger.error("  2. å·²ä¸‹è½½æ•°æ®: python -m qlib.run.data_collector qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn")
        return {}
    
    # ========================================
    # Phase 2: æå–å€™é€‰å› å­
    # ========================================
    logger.info("\n" + "="*60)
    logger.info("  Phase 2: æå–å€™é€‰å› å­")
    logger.info("="*60)
    
    if source == "milvus":
        factors = load_factors_from_milvus()
        if not factors:
            logger.error("Milvusä¸­æ— å› å­ï¼Œè¯·ç¡®ä¿:")
            logger.error("  1. Milvuså·²å¯åŠ¨: docker-compose up -d milvus")
            logger.error("  2. å·²è¿è¡Œå› å­æŒ–æ˜å­˜å‚¨å› å­: python run_factor_mining.py")
            return {}
    elif source == "file" and input_file:
        factors = load_factors_from_json(input_file)
        if not factors:
            logger.error(f"æ–‡ä»¶ä¸­æ— å› å­: {input_file}")
            return {}
    elif source == "library":
        factors = load_factors_from_library(
            factor_sets=library_sets or ["alpha158"],
            max_factors=max_factors * 10,  # åŠ è½½æ›´å¤šç”¨äºç­›é€‰
        )
        if not factors:
            logger.error("å› å­åº“ä¸­æ— å› å­")
            return {}
    else:
        logger.error(f"æœªçŸ¥æ¥æº: {source}ï¼Œæ”¯æŒ: milvus, file, library")
        return {}
    
    logger.info(f"æå–åˆ° {len(factors)} ä¸ªå€™é€‰å› å­")
    
    # æ¸…æ´—å› å­ä»£ç ï¼ˆç§»é™¤ä¸å¿…è¦çš„importï¼Œé€‚é…å­—æ®µåï¼‰
    factors = clean_factors(factors, list(data.columns))
    if not factors:
        logger.error("æ— å¯ç”¨å› å­")
        return {}
    
    # ========================================
    # Phase 3: å› å­ç­›é€‰
    # ========================================
    logger.info("\n" + "="*60)
    logger.info("  Phase 3: å› å­ç­›é€‰")
    logger.info("="*60)
    
    executor = create_sandbox_executor(data)
    
    from alpha_agent.selection import FactorSelector, FactorWrapper
    
    selector = FactorSelector(
        quick_ic_threshold=quick_ic_threshold,
        max_factors=max_factors,
        corr_threshold=corr_threshold,
        enable_cluster=len(factors) > 100,
        n_clusters=min(20, max(5, len(factors) // 5)),
    )
    
    selection_result = selector.select(factors, data, target, executor)
    
    logger.info(selection_result.summary())
    
    # ä¿å­˜ç­›é€‰ç»“æœ
    selected_file = output_path / f"selected_factors_{timestamp}.json"
    selected_data = []
    for f in selection_result.selected_factors:
        selected_data.append({
            'id': f.get('id', ''),
            'name': f.get('name', ''),
            'code': f.get('code', ''),
            'description': f.get('description', ''),
            'ic': float(f.get('ic', 0)),
            'icir': float(f.get('icir', 0)),
            'rank_ic': float(f.get('rank_ic', 0)),
            'category': f.get('category', ''),
            'source': f.get('source', source),
        })
    
    with open(selected_file, 'w', encoding='utf-8') as fp:
        json.dump({'factors': selected_data, 'timestamp': timestamp}, fp, ensure_ascii=False, indent=2)
    logger.info(f"ç­›é€‰ç»“æœå·²ä¿å­˜: {selected_file}")
    
    # å°è£…å› å­
    wrapper = FactorWrapper.from_dict_list(selected_data)
    wrapper.set_executor(executor)
    wrapper.save(output_path / f"factor_wrapper_{timestamp}.json")
    
    # ========================================
    # Phase 4: å›æµ‹
    # ========================================
    backtest_result = None
    
    if run_backtest_flag and len(selected_data) > 0:
        logger.info("\n" + "="*60)
        logger.info("  Phase 4: æ¨¡å‹è®­ç»ƒä¸å›æµ‹")
        logger.info("="*60)
        
        backtest_result = run_backtest(
            wrapper=wrapper,
            data=data,
            target=target,
            train_start=train_start,
            train_end=train_end,
            test_start=test_start,
            test_end=test_end,
            model_type=model_type,
        )
        
        # ä¿å­˜å›æµ‹ç»“æœ
        backtest_file = output_path / f"backtest_result_{timestamp}.json"
        with open(backtest_file, 'w', encoding='utf-8') as fp:
            json.dump(backtest_result.to_dict(), fp, ensure_ascii=False, indent=2)
        logger.info(f"å›æµ‹ç»“æœå·²ä¿å­˜: {backtest_file}")
    
    # ========================================
    # æ€»ç»“
    # ========================================
    elapsed = time.time() - start_time
    
    logger.info("\n" + "="*60)
    logger.info("     âœ… Pipelineå®Œæˆ")
    logger.info("="*60)
    logger.info(f"å€™é€‰å› å­: {len(factors)}")
    logger.info(f"ç­›é€‰å› å­: {len(selected_data)}")
    if backtest_result:
        logger.info(f"å¹´åŒ–æ”¶ç›Š: {backtest_result.annual_return*100:.2f}%")
        logger.info(f"å¤æ™®æ¯”ç‡: {backtest_result.sharpe_ratio:.2f}")
        logger.info(f"ICå‡å€¼: {backtest_result.ic_mean:.4f}")
    logger.info(f"æ€»è€—æ—¶: {elapsed:.1f} ç§’")
    logger.info("="*60)
    
    return {
        'selection': {
            'input_count': len(factors),
            'output_count': len(selected_data),
            'factors': selected_data,
            'file': str(selected_file),
        },
        'backtest': backtest_result.to_dict() if backtest_result else None,
        'elapsed': elapsed,
    }


def load_factors(config: PipelineConfig) -> List[Dict]:
    """
    æ ¹æ®é…ç½®åŠ è½½å› å­
    
    Args:
        config: Pipelineé…ç½®
    
    Returns:
        å› å­åˆ—è¡¨
    """
    if config.source == FactorSource.MILVUS:
        return load_factors_from_milvus(
            host=config.milvus_host,
            port=config.milvus_port,
            collection_name=config.milvus_collection,
            min_ic=config.milvus_min_ic,
        )
    elif config.source == FactorSource.FILE:
        if not config.input_file:
            logger.error("æœªæŒ‡å®šè¾“å…¥æ–‡ä»¶")
            return []
        return load_factors_from_json(config.input_file)
    elif config.source == FactorSource.LIBRARY:
        return load_factors_from_library(
            factor_sets=config.library_sets,
            max_factors=config.max_factors * 10,  # åŠ è½½æ›´å¤šç”¨äºç­›é€‰
        )
    else:
        logger.error(f"æœªçŸ¥å› å­æ¥æº: {config.source}")
        return []


def run_pipeline(config: PipelineConfig) -> Dict:
    """
    ç»Ÿä¸€çš„Pipelineå…¥å£å‡½æ•°
    
    æ ¹æ®é…ç½®è¿è¡Œä¸åŒæ¨¡å¼çš„Pipeline
    
    Args:
        config: Pipelineé…ç½®
    
    Returns:
        ç»“æœå­—å…¸
    """
    # éªŒè¯é…ç½®
    errors = config.validate()
    if errors:
        for error in errors:
            logger.error(f"é…ç½®é”™è¯¯: {error}")
        return {"error": errors}
    
    # æ ¹æ®æ¨¡å¼åˆ†å‘
    if config.mode == PipelineMode.FULL:
        return run_full_pipeline(
            source=config.source.value,
            input_file=config.input_file,
            max_factors=config.max_factors,
            quick_ic_threshold=config.quick_ic_threshold,
            corr_threshold=config.corr_threshold,
            instruments=config.instruments,
            model_type=config.model_type,
            output_dir=config.output_dir,
            run_backtest_flag=True,
            train_start=config.train_start,
            train_end=config.train_end,
            test_start=config.test_start,
            test_end=config.test_end,
            library_sets=config.library_sets,
        )
    
    elif config.mode == PipelineMode.SELECT:
        return run_full_pipeline(
            source=config.source.value,
            input_file=config.input_file,
            max_factors=config.max_factors,
            quick_ic_threshold=config.quick_ic_threshold,
            corr_threshold=config.corr_threshold,
            instruments=config.instruments,
            model_type=config.model_type,
            output_dir=config.output_dir,
            run_backtest_flag=False,
            library_sets=config.library_sets,
        )
    
    elif config.mode == PipelineMode.SELECT_MILVUS:
        return _run_select_milvus(config)
    
    elif config.mode == PipelineMode.BACKTEST:
        return _run_backtest_only(config)
    
    elif config.mode == PipelineMode.QLIB_BENCHMARK:
        return run_qlib_benchmark(
            models=config.qlib_models,
            instruments=config.instruments,
            output_dir=config.output_dir,
        )
    
    elif config.mode == PipelineMode.COMPARE:
        return _run_compare(config)
    
    elif config.mode == PipelineMode.QLIB_CUSTOM:
        return _run_qlib_custom(config)
    
    else:
        logger.error(f"æœªçŸ¥æ¨¡å¼: {config.mode}")
        return {"error": f"æœªçŸ¥æ¨¡å¼: {config.mode}"}


def _run_backtest_only(config: PipelineConfig) -> Dict:
    """ä»…è¿è¡Œå›æµ‹æ¨¡å¼"""
    from alpha_agent.selection import FactorWrapper
    
    # åŠ è½½Qlibæ•°æ®
    data, target = load_qlib_data(
        instruments=config.instruments,
        return_days=config.return_days,
    )
    if data is None:
        return {"error": "Qlibæ•°æ®åŠ è½½å¤±è´¥"}
    
    # åŠ è½½å› å­
    wrapper = FactorWrapper.from_json(config.input_file)
    executor = create_sandbox_executor(data)
    wrapper.set_executor(executor)
    
    # æ‰§è¡Œå›æµ‹
    backtest_result = run_backtest(
        wrapper=wrapper,
        data=data,
        target=target,
        train_start=config.train_start,
        train_end=config.train_end,
        test_start=config.test_start,
        test_end=config.test_end,
        model_type=config.model_type,
    )
    
    return {"backtest": backtest_result.to_dict()}


def _run_qlib_custom(config: PipelineConfig) -> Dict:
    """è‡ªå®šä¹‰å› å­ + Qlibæ¨¡å‹æ¨¡å¼"""
    from alpha_agent.selection import FactorWrapper
    
    wrapper = FactorWrapper.from_json(config.input_file)
    
    return run_qlib_with_custom_factors(
        wrapper=wrapper,
        models=config.qlib_models,
        instruments=config.instruments,
        output_dir=config.output_dir,
    )


def _run_select_milvus(config: PipelineConfig) -> Dict:
    """Milvuså› å­ç­›é€‰æ¨¡å¼ - è°ƒç”¨FactorSelectorè¿›è¡Œå¤šé˜¶æ®µç­›é€‰"""
    result = select_milvus_factors(
        instruments=config.instruments,
        max_factors=config.max_factors,
        output_dir=config.output_dir,
    )
    
    return {
        "selection": {
            "total_input": result.total_input,
            "final_count": result.final_count,
            "selected_factors": result.selected_factors,
            "output_count": result.final_count,
        },
        "status": "success" if result.final_count > 0 else "failed",
    }


def _run_compare(config: PipelineConfig) -> Dict:
    """å› å­é›†å¯¹æ¯”æ¨¡å¼"""
    results = compare_factor_sets(
        factor_sets=config.compare_sets,
        custom_factors_path=config.input_file,
        max_factors_per_set=config.max_factors_per_set,
        instruments=config.instruments,
        model_type=config.model_type,
        train_start=config.train_start,
        train_end=config.train_end,
        test_start=config.test_start,
        test_end=config.test_end,
        output_dir=config.output_dir,
    )
    
    if not results:
        return {"error": "å› å­é›†å¯¹æ¯”å¤±è´¥"}
    
    # è½¬æ¢ç»“æœä¸ºå­—å…¸æ ¼å¼
    return {
        "comparison": {k: v.to_dict() for k, v in results.items()},
        "success_count": sum(1 for r in results.values() if r.status == "success"),
        "total_count": len(results),
    }


def run_selection(
    source: str = "milvus",
    input_file: Optional[str] = None,
    max_factors: int = 30,
    quick_ic_threshold: float = 0.005,
    corr_threshold: float = 0.7,
    instruments: str = "csi300",
    output_dir: str = "output/selection",
    library_sets: Optional[List[str]] = None,
) -> Dict:
    """
    ä»…è¿è¡Œå› å­ç­›é€‰ (ä¸å›æµ‹)
    
    Args:
        source: å› å­æ¥æº ("milvus", "file", "library")
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ (source="file"æ—¶ä½¿ç”¨)
        max_factors: æœ€å¤§å› å­æ•°
        quick_ic_threshold: å¿«é€ŸICé˜ˆå€¼
        corr_threshold: ç›¸å…³æ€§é˜ˆå€¼
        instruments: è‚¡ç¥¨æ± 
        output_dir: è¾“å‡ºç›®å½•
        library_sets: å› å­åº“é›†åˆåˆ—è¡¨ (source="library"æ—¶ä½¿ç”¨)
    
    Returns:
        ç­›é€‰ç»“æœå­—å…¸
    """
    config = PipelineConfig(
        mode=PipelineMode.SELECT,
        source=FactorSource(source),
        input_file=input_file,
        max_factors=max_factors,
        quick_ic_threshold=quick_ic_threshold,
        corr_threshold=corr_threshold,
        instruments=instruments,
        output_dir=output_dir,
        library_sets=library_sets or ["alpha158"],
    )
    return run_pipeline(config)


# ============================================================
# ä¾¿æ·API
# ============================================================

def quick_start(
    factor_sets: List[str] = None,
    max_factors: int = 30,
    instruments: str = "csi300",
    model_type: str = "lgb",
    run_backtest: bool = True,
) -> Dict:
    """
    å¿«é€Ÿå¯åŠ¨å› å­ç­›é€‰Pipeline
    
    ä½¿ç”¨å› å­åº“ä¸­çš„å› å­è¿›è¡Œå¿«é€Ÿç­›é€‰å’Œå›æµ‹ï¼Œé€‚åˆå¿«é€ŸåŸå‹éªŒè¯
    
    Args:
        factor_sets: å› å­é›†åˆ—è¡¨ï¼Œé»˜è®¤ ["alpha158"]
                    å¯é€‰: alpha158, alpha360, worldquant101, gtja191, classic
        max_factors: æœ€å¤§å› å­æ•°
        instruments: è‚¡ç¥¨æ±  (csi300, csi500, all)
        model_type: æ¨¡å‹ç±»å‹ (lgb, linear, ridge)
        run_backtest: æ˜¯å¦è¿è¡Œå›æµ‹
    
    Returns:
        ç»“æœå­—å…¸ï¼ŒåŒ…å«:
        - selection: ç­›é€‰ç»“æœ
        - backtest: å›æµ‹ç»“æœ (å¦‚æœrun_backtest=True)
    
    Example:
        >>> from alpha_agent.run_factor_selection import quick_start
        >>> result = quick_start(
        ...     factor_sets=["alpha158"],
        ...     max_factors=20,
        ...     run_backtest=True
        ... )
        >>> print(f"IC: {result['backtest']['ic_mean']:.4f}")
    """
    config = PipelineConfig(
        mode=PipelineMode.FULL if run_backtest else PipelineMode.SELECT,
        source=FactorSource.LIBRARY,
        library_sets=factor_sets or ["alpha158"],
        max_factors=max_factors,
        instruments=instruments,
        model_type=model_type,
    )
    return run_pipeline(config)


def run_from_json(
    json_path: str,
    max_factors: int = 30,
    instruments: str = "csi300",
    model_type: str = "lgb",
    run_backtest: bool = True,
) -> Dict:
    """
    ä»JSONæ–‡ä»¶åŠ è½½å› å­å¹¶è¿è¡ŒPipeline
    
    Args:
        json_path: JSONæ–‡ä»¶è·¯å¾„
        max_factors: æœ€å¤§å› å­æ•°
        instruments: è‚¡ç¥¨æ± 
        model_type: æ¨¡å‹ç±»å‹
        run_backtest: æ˜¯å¦è¿è¡Œå›æµ‹
    
    Returns:
        ç»“æœå­—å…¸
    """
    config = PipelineConfig(
        mode=PipelineMode.FULL if run_backtest else PipelineMode.SELECT,
        source=FactorSource.FILE,
        input_file=json_path,
        max_factors=max_factors,
        instruments=instruments,
        model_type=model_type,
    )
    return run_pipeline(config)


@dataclass
class ComparisonResult:
    """å› å­é›†å¯¹æ¯”ç»“æœ"""
    name: str
    factor_count: int
    ic_mean: float = 0.0
    icir: float = 0.0
    sharpe_ratio: float = 0.0
    annual_return: float = 0.0
    max_drawdown: float = 0.0
    top_group_return: float = 0.0
    long_short_return: float = 0.0
    elapsed: float = 0.0
    status: str = "success"
    error: str = ""
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸ï¼Œç¡®ä¿æ‰€æœ‰æ•°å€¼ç±»å‹å¯JSONåºåˆ—åŒ–"""
        d = asdict(self)
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        for k, v in d.items():
            if hasattr(v, 'item'):  # numpyæ•°å€¼ç±»å‹
                d[k] = v.item()
            elif isinstance(v, float):
                d[k] = float(v)
        return d


def compare_factor_sets(
    factor_sets: List[str] = None,
    custom_factors_path: Optional[str] = None,
    max_factors_per_set: int = 50,
    instruments: str = "csi300",
    model_type: str = "lgb",
    train_start: str = "2022-01-01",
    train_end: str = "2022-12-31",
    test_start: str = "2023-01-01",
    test_end: str = "2023-12-31",
    output_dir: str = "output/comparison",
) -> Dict[str, ComparisonResult]:
    """
    å¯¹æ¯”ä¸åŒå› å­é›†åœ¨åŒä¸€æ¨¡å‹ä¸Šçš„å›æµ‹æ•ˆæœ
    
    Args:
        factor_sets: è¦å¯¹æ¯”çš„å› å­é›†åˆ—è¡¨
                    å¯é€‰: alpha158, alpha360, worldquant101, gtja191, classic
                    é»˜è®¤: ["alpha158", "worldquant101", "gtja191"]
        custom_factors_path: è‡ªå®šä¹‰å› å­JSONæ–‡ä»¶è·¯å¾„ (å¯é€‰)
        max_factors_per_set: æ¯ä¸ªå› å­é›†ä½¿ç”¨çš„æœ€å¤§å› å­æ•°
        instruments: è‚¡ç¥¨æ± 
        model_type: æ¨¡å‹ç±»å‹ (lgb, linear, ridge)
        train_start: è®­ç»ƒå¼€å§‹æ—¥æœŸ
        train_end: è®­ç»ƒç»“æŸæ—¥æœŸ
        test_start: æµ‹è¯•å¼€å§‹æ—¥æœŸ
        test_end: æµ‹è¯•ç»“æŸæ—¥æœŸ
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        Dict[str, ComparisonResult]: æ¯ä¸ªå› å­é›†çš„å¯¹æ¯”ç»“æœ
    
    Example:
        >>> from alpha_agent.run_factor_selection import compare_factor_sets
        >>> results = compare_factor_sets(
        ...     factor_sets=["alpha158", "worldquant101"],
        ...     custom_factors_path="output/selection/selected_factors.json",
        ...     model_type="lgb"
        ... )
        >>> for name, r in results.items():
        ...     print(f"{name}: IC={r.ic_mean:.4f}, Sharpe={r.sharpe_ratio:.2f}")
    """
    logger.info("="*70)
    logger.info("     ğŸ“Š å› å­é›†å¯¹æ¯”æµ‹è¯•")
    logger.info("="*70)
    
    if factor_sets is None:
        factor_sets = ["alpha158", "worldquant101", "gtja191"]
    
    logger.info(f"å› å­é›†: {factor_sets}")
    if custom_factors_path:
        logger.info(f"è‡ªå®šä¹‰å› å­: {custom_factors_path}")
    logger.info(f"æ¨¡å‹: {model_type}")
    logger.info(f"è®­ç»ƒæœŸ: {train_start} ~ {train_end}")
    logger.info(f"æµ‹è¯•æœŸ: {test_start} ~ {test_end}")
    logger.info("="*70)
    
    # 1. åŠ è½½Qlibæ•°æ® (åªåŠ è½½ä¸€æ¬¡)
    logger.info("\nğŸ“Š Step 1: åŠ è½½Qlibæ•°æ®")
    data, target = load_qlib_data(instruments=instruments)
    if data is None:
        logger.error("Qlibæ•°æ®åŠ è½½å¤±è´¥")
        return {}
    
    # æ·»åŠ æ´¾ç”Ÿå­—æ®µï¼ˆæ”¯æŒå¤§æ¨¡å‹ç”Ÿæˆçš„å› å­ä½¿ç”¨market_cap, market_retç­‰ï¼‰
    data = add_derived_fields(data)
    logger.info(f"å·²æ·»åŠ æ´¾ç”Ÿå­—æ®µ: {list(data.columns)[-7:]}")
    
    executor = create_sandbox_executor(data)
    
    # 2. å‡†å¤‡ç»“æœ
    results: Dict[str, ComparisonResult] = {}
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # 3. æµ‹è¯•æ¯ä¸ªå› å­é›†
    all_factor_sets = list(factor_sets)
    if custom_factors_path:
        all_factor_sets.append("custom")
    
    for i, set_name in enumerate(all_factor_sets, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"  [{i}/{len(all_factor_sets)}] æµ‹è¯•å› å­é›†: {set_name}")
        logger.info("="*60)
        
        start_time = time.time()
        
        try:
            # åŠ è½½å› å­
            if set_name == "custom":
                factors = load_factors_from_json(custom_factors_path)
                display_name = f"custom ({Path(custom_factors_path).stem})"
            elif set_name == "milvus":
                # ä»Milvusæ•°æ®åº“åŠ è½½å› å­ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ï¼‰
                from alpha_agent.config.settings import vector_db_config
                factors = load_factors_from_milvus(
                    host=vector_db_config.host,
                    port=vector_db_config.port,
                    collection_name=vector_db_config.collection_name,
                    min_ic=0.01,  # åªåŠ è½½IC > 0.01çš„å› å­
                )
                if factors and max_factors_per_set:
                    factors = factors[:max_factors_per_set]
                display_name = f"milvus ({len(factors) if factors else 0})"
            elif set_name == "milvus-selected":
                # ä»MilvusåŠ è½½å› å­å¹¶é€šè¿‡FactorSelectorè¿›è¡Œå¤šé˜¶æ®µç­›é€‰
                logger.info("ä½¿ç”¨FactorSelectorè¿›è¡Œå¤šé˜¶æ®µç­›é€‰...")
                from alpha_agent.config.settings import vector_db_config
                milvus_factors = load_factors_from_milvus(
                    host=vector_db_config.host,
                    port=vector_db_config.port,
                    collection_name=vector_db_config.collection_name,
                )
                if milvus_factors:
                    # æ¸…æ´—å› å­
                    milvus_factors = clean_factors(milvus_factors, list(data.columns))
                    # ä½¿ç”¨FactorSelectorç­›é€‰
                    selector = FactorSelector(max_factors=max_factors_per_set or 30)
                    selection_result = selector.select(
                        factors=milvus_factors,
                        data=data,
                        target=target,
                        sandbox_executor=executor,
                    )
                    factors = selection_result.selected_factors
                    display_name = f"milvus-selected ({len(factors) if factors else 0})"
                else:
                    factors = []
                    display_name = "milvus-selected (0)"
            else:
                factors = load_factors_from_library(
                    factor_sets=[set_name],
                    max_factors=max_factors_per_set,
                )
                display_name = set_name
            
            if not factors:
                logger.warning(f"å› å­é›† {set_name} æ— å¯ç”¨å› å­")
                results[set_name] = ComparisonResult(
                    name=display_name,
                    factor_count=0,
                    status="error",
                    error="æ— å¯ç”¨å› å­",
                )
                continue
            
            # æ¸…æ´—å› å­ä»£ç ï¼ˆmilvus-selectedå·²ç»æ¸…æ´—è¿‡ï¼‰
            if set_name != "milvus-selected":
                factors = clean_factors(factors, list(data.columns))
            if not factors:
                logger.warning(f"å› å­é›† {set_name} æ— å¯ç”¨å› å­")
                results[set_name] = ComparisonResult(
                    name=display_name,
                    factor_count=0,
                    status="error",
                    error="è¿‡æ»¤åæ— å¯ç”¨å› å­",
                )
                continue
            
            logger.info(f"å› å­æ•°: {len(factors)}")
            
            # åˆ›å»ºFactorWrapper
            from alpha_agent.selection import FactorWrapper
            wrapper = FactorWrapper.from_dict_list(factors[:max_factors_per_set])
            wrapper.set_executor(executor)
            
            # è¿è¡Œå›æµ‹
            backtest_result = run_backtest(
                wrapper=wrapper,
                data=data,
                target=target,
                train_start=train_start,
                train_end=train_end,
                test_start=test_start,
                test_end=test_end,
                model_type=model_type,
            )
            
            elapsed = time.time() - start_time
            
            results[set_name] = ComparisonResult(
                name=display_name,
                factor_count=len(factors[:max_factors_per_set]),
                ic_mean=backtest_result.ic_mean,
                icir=backtest_result.icir,
                sharpe_ratio=backtest_result.sharpe_ratio,
                annual_return=backtest_result.annual_return,
                max_drawdown=backtest_result.max_drawdown,
                top_group_return=backtest_result.top_group_return,
                long_short_return=backtest_result.long_short_return,
                elapsed=elapsed,
                status="success",
            )
            
            logger.info(f"âœ“ {display_name}: IC={backtest_result.ic_mean:.4f}, "
                       f"ICIR={backtest_result.icir:.2f}, "
                       f"Sharpe={backtest_result.sharpe_ratio:.2f}")
            
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"å› å­é›† {set_name} æµ‹è¯•å¤±è´¥: {e}")
            results[set_name] = ComparisonResult(
                name=set_name,
                factor_count=0,
                elapsed=elapsed,
                status="error",
                error=str(e),
            )
    
    # 4. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    logger.info("\n" + "="*70)
    logger.info("     ğŸ“ˆ å› å­é›†å¯¹æ¯”ç»“æœ")
    logger.info("="*70)
    
    # æŒ‰ICæ’åº
    sorted_results = sorted(
        [(k, v) for k, v in results.items() if v.status == "success"],
        key=lambda x: x[1].ic_mean,
        reverse=True
    )
    
    # æ‰“å°è¡¨æ ¼
    print(f"\n{'å› å­é›†':<25} {'å› å­æ•°':>6} {'IC':>8} {'ICIR':>8} {'Sharpe':>8} {'å¹´åŒ–æ”¶ç›Š':>10} {'å¤šç©ºæ”¶ç›Š':>10}")
    print("-" * 85)
    
    for name, r in sorted_results:
        print(f"{r.name:<25} {r.factor_count:>6} {r.ic_mean:>8.4f} {r.icir:>8.2f} "
              f"{r.sharpe_ratio:>8.2f} {r.annual_return*100:>9.2f}% {r.long_short_return*100:>9.2f}%")
    
    print("-" * 85)
    
    # æ‰“å°å¤±è´¥çš„
    failed = [(k, v) for k, v in results.items() if v.status != "success"]
    if failed:
        print("\nå¤±è´¥çš„å› å­é›†:")
        for name, r in failed:
            print(f"  - {name}: {r.error}")
    
    # 5. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜JSON
    result_file = output_path / f"comparison_{timestamp}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump({k: v.to_dict() for k, v in results.items()}, f, ensure_ascii=False, indent=2)
    logger.info(f"\nç»“æœå·²ä¿å­˜: {result_file}")
    
    # ä¿å­˜CSV
    csv_file = output_path / f"comparison_{timestamp}.csv"
    rows = []
    for name, r in results.items():
        rows.append({
            'factor_set': r.name,
            'factor_count': r.factor_count,
            'ic_mean': r.ic_mean,
            'icir': r.icir,
            'sharpe_ratio': r.sharpe_ratio,
            'annual_return': r.annual_return,
            'max_drawdown': r.max_drawdown,
            'top_group_return': r.top_group_return,
            'long_short_return': r.long_short_return,
            'elapsed': r.elapsed,
            'status': r.status,
        })
    pd.DataFrame(rows).to_csv(csv_file, index=False)
    logger.info(f"CSVå·²ä¿å­˜: {csv_file}")
    
    # æ‰¾å‡ºæœ€ä½³å› å­é›†
    if sorted_results:
        best_name, best_result = sorted_results[0]
        logger.info(f"\nğŸ† æœ€ä½³å› å­é›†: {best_result.name}")
        logger.info(f"   ICå‡å€¼: {best_result.ic_mean:.4f}")
        logger.info(f"   å¤æ™®æ¯”ç‡: {best_result.sharpe_ratio:.2f}")
        logger.info(f"   å¹´åŒ–æ”¶ç›Š: {best_result.annual_return*100:.2f}%")
    
    logger.info("\n" + "="*70)
    
    return results


# ============================================================
# Milvus å› å­ç­›é€‰
# ============================================================

def select_milvus_factors(
    instruments: str = "csi300",
    max_factors: int = 50,
    output_dir: str = "output/selection",
) -> SelectionResult:
    """
    ä» Milvus åŠ è½½å› å­å¹¶è¿›è¡Œå¤šé˜¶æ®µç­›é€‰
    
    Pipeline:
    1. ä» Milvus åŠ è½½æ‰€æœ‰å› å­
    2. åŠ è½½ Qlib æ•°æ®
    3. è°ƒç”¨ FactorSelector è¿›è¡Œç­›é€‰
    4. ä¿å­˜ç­›é€‰ç»“æœ
    
    Args:
        instruments: è‚¡ç¥¨æ± 
        max_factors: æœ€ç»ˆé€‰æ‹©çš„å› å­æ•°ä¸Šé™
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        SelectionResult
    """
    logger.info("="*70)
    logger.info("     ğŸ“Š Milvus å› å­ç­›é€‰")
    logger.info("="*70)
    
    # 1. åŠ è½½ Qlib æ•°æ®
    logger.info("\nğŸ“Š Step 1: åŠ è½½ Qlib æ•°æ®")
    data, target = load_qlib_data(instruments=instruments)
    if data is None:
        logger.error("Qlib æ•°æ®åŠ è½½å¤±è´¥")
        return SelectionResult()
    
    data = add_derived_fields(data)
    logger.info(f"æ•°æ®ç»´åº¦: {data.shape}, æ´¾ç”Ÿå­—æ®µå·²æ·»åŠ ")
    
    # 2. ä» Milvus åŠ è½½å› å­
    logger.info("\nğŸ“Š Step 2: ä» Milvus åŠ è½½å› å­")
    from alpha_agent.config.settings import vector_db_config
    factors = load_factors_from_milvus(
        host=vector_db_config.host,
        port=vector_db_config.port,
        collection_name=vector_db_config.collection_name,
    )
    
    if not factors:
        logger.error("Milvus ä¸­æ— å› å­")
        return SelectionResult()
    
    logger.info(f"åŠ è½½ {len(factors)} ä¸ªå› å­")
    
    # 3. æ¸…æ´—å› å­ä»£ç 
    logger.info("\nğŸ“Š Step 3: æ¸…æ´—å› å­ä»£ç ")
    factors = clean_factors(factors, list(data.columns))
    logger.info(f"æ¸…æ´—å: {len(factors)} ä¸ªå› å­")
    
    # 4. åˆ›å»ºæ²™ç®±æ‰§è¡Œå™¨
    executor = create_sandbox_executor(data)
    
    # 5. å› å­ç­›é€‰
    logger.info("\nğŸ“Š Step 4: å¼€å§‹å› å­ç­›é€‰")
    selector = FactorSelector(max_factors=max_factors)
    
    result = selector.select(
        factors=factors,
        data=data,
        target=target,
        sandbox_executor=executor,
    )
    
    # 6. ä¿å­˜ç»“æœ
    logger.info("\nğŸ“Š Step 5: ä¿å­˜ç­›é€‰ç»“æœ")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # æ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
    def make_serializable(obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼"""
        import pandas as pd
        import numpy as np
        if isinstance(obj, dict):
            # å¤„ç†å­—å…¸é”®å¯èƒ½æ˜¯å…ƒç»„çš„æƒ…å†µ
            result = {}
            for k, v in obj.items():
                if isinstance(k, tuple):
                    k = str(k)  # å…ƒç»„é”®è½¬ä¸ºå­—ç¬¦ä¸²
                result[k] = make_serializable(v)
            return result
        elif isinstance(obj, list):
            return [make_serializable(v) for v in obj]
        elif isinstance(obj, tuple):
            return list(obj)
        elif isinstance(obj, (pd.Series, pd.DataFrame)):
            # Series/DataFrame è½¬ä¸ºç®€å•åˆ—è¡¨æˆ–åµŒå¥—åˆ—è¡¨
            if isinstance(obj, pd.Series):
                return obj.tolist()
            return obj.values.tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif hasattr(obj, '__dict__') and not isinstance(obj, type):
            return make_serializable(obj.__dict__)
        return obj
    
    # ä¿å­˜é€‰ä¸­å› å­
    selected_file = output_path / f"selected_factors_{timestamp}.json"
    with open(selected_file, 'w', encoding='utf-8') as f:
        import json
        serializable_factors = make_serializable(result.selected_factors)
        json.dump(serializable_factors, f, ensure_ascii=False, indent=2)
    logger.info(f"é€‰ä¸­å› å­å·²ä¿å­˜: {selected_file}")
    
    # ä¿å­˜å› å­è¯¦æƒ…
    details_file = output_path / f"factor_details_{timestamp}.json"
    with open(details_file, 'w', encoding='utf-8') as f:
        serializable_details = make_serializable(result.factor_details)
        json.dump(serializable_details, f, ensure_ascii=False, indent=2)
    logger.info(f"å› å­è¯¦æƒ…å·²ä¿å­˜: {details_file}")
    
    # ç”Ÿæˆ Qlib é…ç½®
    qlib_config = generate_qlib_config(result.selected_factors)
    config_file = output_path / f"qlib_factors_{timestamp}.yaml"
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(qlib_config)
    logger.info(f"Qlibé…ç½®å·²ä¿å­˜: {config_file}")
    
    logger.info("="*70)
    logger.info(f"âœ… å› å­ç­›é€‰å®Œæˆ: {result.total_input} â†’ {result.final_count}")
    logger.info("="*70)
    
    return result


def generate_qlib_config(factors: List[Dict]) -> str:
    """ç”ŸæˆQlibå› å­é…ç½®"""
    lines = [
        "# Qlibå› å­é…ç½® - ç”±å› å­ç­›é€‰ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ",
        f"# ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}",
        f"# å› å­æ•°é‡: {len(factors)}",
        "",
        "data_handler_config:",
        "  class: Alpha158",
        "  module_path: qlib.contrib.data.handler",
        "  kwargs:",
        "    instruments: csi300",
        "    start_time: '2018-01-01'",
        "    end_time: '2023-12-31'",
        "    fit_start_time: '2018-01-01'",
        "    fit_end_time: '2021-12-31'",
        "",
        "# è‡ªå®šä¹‰å› å­è¡¨è¾¾å¼",
        "custom_factors:",
    ]
    
    for f in factors:
        name = f.get('name', f.get('id', 'unknown'))
        code = f.get('code', '')
        ic = f.get('ic', 0)
        
        # è½¬æ¢ä¸ºQlibè¡¨è¾¾å¼æ ¼å¼
        qlib_expr = code.replace('df["', '$').replace('"]', '')
        qlib_expr = qlib_expr.replace("df['", '$').replace("']", '')
        
        lines.append(f"  - name: {name}")
        lines.append(f"    expression: \"{qlib_expr}\"")
        lines.append(f"    ic: {ic:.4f}")
        lines.append("")
    
    return "\n".join(lines)


# ============================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="å› å­ç­›é€‰ä¸å›æµ‹Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å®Œæ•´Pipeline (ä»Milvusæå–å› å­ â†’ ç­›é€‰ â†’ å›æµ‹)
  python run_factor_selection.py --mode full
  
  # ä»å› å­åº“åŠ è½½ Alpha158 å› å­è¿›è¡Œç­›é€‰
  python run_factor_selection.py --mode full --source library --library-sets alpha158
  
  # ä»æ–‡ä»¶åŠ è½½å› å­è¿›è¡Œç­›é€‰
  python run_factor_selection.py --mode select --source file --input factors.json
  
  # è‡ªå®šä¹‰æ—¶é—´æ®µå›æµ‹
  python run_factor_selection.py --mode backtest --input output/selected_factors.json \
      --train-start 2021-01-01 --train-end 2022-06-30 --test-start 2022-07-01 --test-end 2023-12-31
  
  # Qlibå¤šæ¨¡å‹åŸºå‡†æµ‹è¯•
  python run_factor_selection.py --mode qlib-benchmark --qlib-models lgb,xgb,linear
  
  # è‡ªå®šä¹‰å› å­ + Qlibæ¨¡å‹
  python run_factor_selection.py --mode qlib-custom --input output/selected_factors.json
  
  # å› å­é›†å¯¹æ¯”æµ‹è¯•
  python run_factor_selection.py --mode compare --compare-sets alpha158,worldquant101,gtja191
  
  # å¯¹æ¯”å†…ç½®å› å­é›†ä¸è‡ªå®šä¹‰ç­›é€‰å› å­
  python run_factor_selection.py --mode compare --compare-sets alpha158 --input output/selection/selected_factors.json
        """
    )
    
    # æ¨¡å¼
    parser.add_argument(
        "--mode",
        choices=["full", "select", "select-milvus", "backtest", "compare", "qlib-benchmark", "qlib-custom"],
        default="full",
        help="è¿è¡Œæ¨¡å¼: full(å®Œæ•´), select(ä»…ç­›é€‰), select-milvus(ç­›é€‰Milvuså› å­), backtest(å›æµ‹), compare(å› å­é›†å¯¹æ¯”), qlib-benchmark(Qlibå¤šæ¨¡å‹), qlib-custom(è‡ªå®šä¹‰å› å­+Qlib) (é»˜è®¤: full)"
    )
    
    # æ•°æ®æ¥æº
    parser.add_argument(
        "--source", "-s",
        choices=["milvus", "file", "library"],
        default="milvus",
        help="å› å­æ¥æº: milvus(å‘é‡æ•°æ®åº“), file(JSONæ–‡ä»¶), library(å› å­åº“) (é»˜è®¤: milvus)"
    )
    
    parser.add_argument(
        "--input", "-i",
        type=str,
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„ (mode=backtestæˆ–source=fileæ—¶ä½¿ç”¨)"
    )
    
    parser.add_argument(
        "--library-sets",
        type=str,
        default="alpha158",
        help="å› å­åº“é›†åˆ (é€—å·åˆ†éš”): alpha158,alpha360,worldquant101,gtja191,classic (é»˜è®¤: alpha158)"
    )
    
    parser.add_argument(
        "--compare-sets",
        type=str,
        default="alpha158,worldquant101,gtja191",
        help="å¯¹æ¯”æ¨¡å¼çš„å› å­é›†åˆ—è¡¨ (é€—å·åˆ†éš”): alpha158,worldquant101,gtja191,milvus,milvus-selected,custom (é»˜è®¤: alpha158,worldquant101,gtja191)"
    )
    
    parser.add_argument(
        "--max-factors-per-set",
        type=int,
        default=50,
        help="å¯¹æ¯”æ¨¡å¼ä¸­æ¯ä¸ªå› å­é›†çš„æœ€å¤§å› å­æ•° (é»˜è®¤: 50)"
    )
    
    # ç­›é€‰å‚æ•°
    parser.add_argument(
        "--max-factors", "-m",
        type=int,
        default=30,
        help="æœ€å¤§å› å­æ•° (é»˜è®¤: 30)"
    )
    
    parser.add_argument(
        "--quick-ic",
        type=float,
        default=0.005,
        help="å¿«é€ŸICé˜ˆå€¼ (é»˜è®¤: 0.005)"
    )
    
    parser.add_argument(
        "--corr-threshold", "-c",
        type=float,
        default=0.7,
        help="ç›¸å…³æ€§é˜ˆå€¼ (é»˜è®¤: 0.7)"
    )
    
    # å›æµ‹å‚æ•°
    parser.add_argument(
        "--model",
        choices=["lgb", "linear", "ridge"],
        default="lgb",
        help="ç®€å•å›æµ‹æ¨¡å‹ç±»å‹: lgb(LightGBM), linear, ridge (é»˜è®¤: lgb)"
    )
    
    parser.add_argument(
        "--qlib-models",
        type=str,
        default="lgb,lgb_light,xgb,linear",
        help="Qlibå›æµ‹æ¨¡å‹åˆ—è¡¨ (é€—å·åˆ†éš”): lgb,xgb,catboost,linear,mlp,lstm,gru,transformer (é»˜è®¤: lgb,lgb_light,xgb,linear)"
    )
    
    # æ•°æ®å‚æ•°
    parser.add_argument(
        "--instruments",
        default="csi300",
        help="è‚¡ç¥¨æ±  (é»˜è®¤: csi300)"
    )
    
    # æ—¶é—´æ®µå‚æ•°
    parser.add_argument(
        "--train-start",
        type=str,
        default="2022-01-01",
        help="è®­ç»ƒå¼€å§‹æ—¥æœŸ (é»˜è®¤: 2022-01-01)"
    )
    
    parser.add_argument(
        "--train-end",
        type=str,
        default="2022-12-31",
        help="è®­ç»ƒç»“æŸæ—¥æœŸ (é»˜è®¤: 2022-12-31)"
    )
    
    parser.add_argument(
        "--test-start",
        type=str,
        default="2023-01-01",
        help="æµ‹è¯•å¼€å§‹æ—¥æœŸ (é»˜è®¤: 2023-01-01)"
    )
    
    parser.add_argument(
        "--test-end",
        type=str,
        default="2023-12-31",
        help="æµ‹è¯•ç»“æŸæ—¥æœŸ (é»˜è®¤: 2023-12-31)"
    )
    
    # è¾“å‡º
    parser.add_argument(
        "--output", "-o",
        default="output/selection",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: output/selection)"
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # ç»Ÿä¸€ä½¿ç”¨PipelineConfigç®¡ç†æ‰€æœ‰æ¨¡å¼
    try:
        config = PipelineConfig.from_args(args)
    except ValueError as e:
        logger.error(f"é…ç½®è§£æå¤±è´¥: {e}")
        return 1
    
    # éªŒè¯é…ç½®
    errors = config.validate()
    if errors:
        for error in errors:
            logger.error(f"é…ç½®é”™è¯¯: {error}")
        return 1
    
    # è¿è¡ŒPipeline
    result = run_pipeline(config)
    
    # å¤„ç†é”™è¯¯
    if "error" in result:
        logger.error(f"Pipelineæ‰§è¡Œå¤±è´¥: {result['error']}")
        return 1
    
    # æ ¹æ®æ¨¡å¼å¤„ç†ç»“æœ
    if config.mode == PipelineMode.COMPARE:
        # å¯¹æ¯”æ¨¡å¼
        comparison = result.get('comparison', {})
        success_count = result.get('success_count', 0)
        total_count = result.get('total_count', 0)
        
        if success_count == 0:
            logger.error("æ‰€æœ‰å› å­é›†æµ‹è¯•éƒ½å¤±è´¥")
            return 1
        
        logger.info(f"\nâœ… å› å­é›†å¯¹æ¯”å®Œæˆï¼ŒæˆåŠŸæµ‹è¯• {success_count}/{total_count} ä¸ªå› å­é›†")
        return 0
    
    if config.mode == PipelineMode.SELECT_MILVUS:
        # Milvuså› å­ç­›é€‰æ¨¡å¼
        selection = result.get('selection', {})
        total_input = selection.get('total_input', 0)
        final_count = selection.get('final_count', 0)
        
        if final_count > 0:
            logger.info(f"\nâœ… Milvuså› å­ç­›é€‰å®Œæˆ: {total_input} â†’ {final_count}")
            return 0
        else:
            logger.warning("\nâš ï¸ æœªç­›é€‰å‡ºæœ‰æ•ˆå› å­")
            return 1
    
    # å…¶ä»–æ¨¡å¼
    selection = result.get('selection', {})
    backtest = result.get('backtest', {})
    
    if selection.get('output_count', 0) > 0:
        logger.info("\nâœ… å› å­ç­›é€‰å®Œæˆ")
    
    if backtest:
        ic = backtest.get('ic_mean', 0)
        if abs(ic) > 0.01:
            logger.info("âœ… å›æµ‹å®Œæˆï¼Œæ¨¡å‹æœ‰æ•ˆ")
            return 0
        else:
            logger.warning("âš ï¸ å›æµ‹å®Œæˆï¼Œä½†ICè¾ƒä½")
            return 0
    
    if selection.get('output_count', 0) == 0:
        logger.warning("\nâš ï¸ æœªç­›é€‰å‡ºæœ‰æ•ˆå› å­")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
