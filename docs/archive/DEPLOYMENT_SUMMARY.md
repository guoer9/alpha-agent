# Qwen3-8B é‡‘èæ–°é—»åˆ†ç±»æ¨¡å‹éƒ¨ç½²æ€»ç»“

## ğŸ“Š éƒ¨ç½²å®ŒæˆçŠ¶æ€

### âœ… å·²å®Œæˆä»»åŠ¡

1. **æ¨¡å‹ä¸‹è½½** - Qwen3-8BåŸºç¡€æ¨¡å‹ (16GB)
2. **LoRAåˆå¹¶** - å¾®è°ƒæ¨¡å‹ä¸åŸºç¡€æ¨¡å‹åˆå¹¶
3. **8-bité‡åŒ–** - æ¨¡å‹å‹ç¼©è‡³çº¦7GBæ˜¾å­˜å ç”¨
4. **å¹¶å‘é™åˆ¶** - é…ç½®æœ€ä¼˜å¹¶å‘è¯·æ±‚æ•°
5. **APIæ¥å£** - å®Œæ•´çš„é‡‘èæ–°é—»åˆ†ç±»API
6. **ç”Ÿäº§éƒ¨ç½²** - Gunicornç”Ÿäº§çº§æœåŠ¡å™¨

## ğŸ–¥ï¸ æœºå™¨é…ç½®

- **GPU**: NVIDIA GeForce RTX 3080 (10GBæ˜¾å­˜)
- **é©±åŠ¨**: 580.95.05, CUDA 13.0
- **å†…å­˜**: 31GB
- **CPU**: 16æ ¸
- **æ˜¾å­˜ä½¿ç”¨**: 7GB (æ¨¡å‹) + 3GB (æ¨ç†ç¼“å†²)

## ğŸš€ æœåŠ¡é…ç½®

### å¹¶å‘é™åˆ¶ï¼ˆå·²ä¼˜åŒ–ï¼‰
- **æœ€å¤§å¹¶å‘è¯·æ±‚**: 3ä¸ª
- **è¯·æ±‚é˜Ÿåˆ—å¤§å°**: 10ä¸ª
- **é€Ÿç‡é™åˆ¶**: 10è¯·æ±‚/åˆ†é’Ÿ (æ¯IP)
- **æœ€å¤§Tokenæ•°**: 512
- **è¶…æ—¶æ—¶é—´**: 120ç§’

### æ€§èƒ½æŒ‡æ ‡
- **å•è¯·æ±‚å“åº”æ—¶é—´**: 2-5ç§’
- **ç†è®ºååé‡**: çº¦60è¯·æ±‚/åˆ†é’Ÿ
- **å®é™…ååé‡**: çº¦40-50è¯·æ±‚/åˆ†é’Ÿ

## ğŸ“ æ–‡ä»¶ç»“æ„

```
/home/zch/qwen_vllm/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ Qwen/
â”‚   â”‚   â””â”€â”€ Qwen3-8B/                    # åŸºç¡€æ¨¡å‹ (16GB)
â”‚   â”œâ”€â”€ qwen-news-classifier/            # åŸå§‹LoRAé€‚é…å™¨
â”‚   â”‚   â”œâ”€â”€ checkpoint-6500/
â”‚   â”‚   â””â”€â”€ checkpoint-10005/            # æœ€ä½³checkpoint
â”‚   â””â”€â”€ qwen-news-classifier-merged/     # åˆå¹¶åçš„æ¨¡å‹ (16GB)
â”‚
â”œâ”€â”€ deploy_with_transformers.py         # åŸºç¡€éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ deploy_with_limits.py               # å¸¦å¹¶å‘é™åˆ¶çš„éƒ¨ç½²è„šæœ¬ â­
â”œâ”€â”€ start_deploy.sh                     # å¼€å‘ç¯å¢ƒå¯åŠ¨è„šæœ¬
â”œâ”€â”€ start_production.sh                 # ç”Ÿäº§ç¯å¢ƒå¯åŠ¨è„šæœ¬ â­
â”œâ”€â”€ setup_deploy_env.sh                 # ç¯å¢ƒå®‰è£…è„šæœ¬
â”‚
â”œâ”€â”€ test_api.py                         # APIæµ‹è¯•è„šæœ¬
â”œâ”€â”€ API_USAGE.md                        # APIä½¿ç”¨æ–‡æ¡£ â­
â”œâ”€â”€ CONCURRENCY_CONFIG.md               # å¹¶å‘é…ç½®è¯´æ˜ â­
â””â”€â”€ DEPLOYMENT_SUMMARY.md               # æœ¬æ–‡æ¡£
```

## ğŸ¯ å¿«é€Ÿå¯åŠ¨

### æ–¹æ³•1: ç”Ÿäº§ç¯å¢ƒï¼ˆæ¨èï¼‰

```bash
cd /home/zch/qwen_vllm

# å¯åŠ¨æœåŠ¡ï¼ˆå¸¦å¹¶å‘é™åˆ¶å’Œé€Ÿç‡é™åˆ¶ï¼‰
bash start_production.sh
```

### æ–¹æ³•2: å¼€å‘ç¯å¢ƒ

```bash
cd /home/zch/qwen_vllm

# å¯åŠ¨æœåŠ¡ï¼ˆç®€å•æ¨¡å¼ï¼‰
bash start_deploy.sh
```

### æ–¹æ³•3: æ‰‹åŠ¨å¯åŠ¨

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate vllm-deploy

# å¯åŠ¨æœåŠ¡
python deploy_with_limits.py \
    --model-path ./models/qwen-news-classifier-merged \
    --host 0.0.0.0 \
    --port 8000
```

## ğŸ”Œ APIæ¥å£

### æœåŠ¡åœ°å€
- **ä¸»åœ°å€**: http://localhost:8000
- **å¥åº·æ£€æŸ¥**: http://localhost:8000/health
- **ç»Ÿè®¡ä¿¡æ¯**: http://localhost:8000/stats
- **æ¨¡å‹åˆ—è¡¨**: http://localhost:8000/v1/models

### é‡‘èæ–°é—»åˆ†ç±»æ¥å£

**ç«¯ç‚¹**: `POST /v1/chat/completions`

**è¯·æ±‚ç¤ºä¾‹**:
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "è¯·åˆ†æä»¥ä¸‹æ–°é—»çš„ç±»åˆ«ï¼šå¤®è¡Œå®£å¸ƒé™å‡†0.5ä¸ªç™¾åˆ†ç‚¹"
      }
    ],
    "max_tokens": 100,
    "temperature": 0.3
  }'
```

**Pythonå®¢æˆ·ç«¯**:
```python
import requests

def classify_news(news_text):
    url = "http://localhost:8000/v1/chat/completions"
    payload = {
        "messages": [
            {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹æ–°é—»çš„ç±»åˆ«ï¼š{news_text}"}
        ],
        "max_tokens": 100,
        "temperature": 0.3
    }
    response = requests.post(url, json=payload)
    return response.json()['choices'][0]['message']['content']

# ä½¿ç”¨
result = classify_news("å¤®è¡Œå®£å¸ƒé™å‡†0.5ä¸ªç™¾åˆ†ç‚¹")
print(result)
```

## ğŸ“Š ç›‘æ§å‘½ä»¤

### æŸ¥çœ‹GPUçŠ¶æ€
```bash
watch -n 1 nvidia-smi
```

### æŸ¥çœ‹æœåŠ¡ç»Ÿè®¡
```bash
curl http://localhost:8000/stats
```

### æŸ¥çœ‹å¥åº·çŠ¶æ€
```bash
curl http://localhost:8000/health
```

### æµ‹è¯•API
```bash
python test_api.py
```

## ğŸ”§ ç»´æŠ¤æ“ä½œ

### é‡å¯æœåŠ¡
```bash
# åœæ­¢æœåŠ¡ (Ctrl+C æˆ– killè¿›ç¨‹)
pkill -f deploy_with_limits

# é‡æ–°å¯åŠ¨
bash start_production.sh
```

### æŸ¥çœ‹æ—¥å¿—
æœåŠ¡æ—¥å¿—ç›´æ¥è¾“å‡ºåˆ°ç»ˆç«¯ï¼ŒåŒ…å«ï¼š
- è¯·æ±‚å¤„ç†ä¿¡æ¯
- é”™è¯¯ä¿¡æ¯
- æ€§èƒ½ç»Ÿè®¡

### æ¸…ç†æ˜¾å­˜
```bash
# å¦‚æœé‡åˆ°æ˜¾å­˜ä¸è¶³
pkill -f python
nvidia-smi

# é‡å¯æœåŠ¡
bash start_production.sh
```

## âš™ï¸ é…ç½®è°ƒæ•´

### ä¿®æ”¹å¹¶å‘æ•°

ç¼–è¾‘ `deploy_with_limits.py`:
```python
MAX_CONCURRENT_REQUESTS = 3  # æ”¹ä¸º2-4ä¹‹é—´çš„å€¼
```

### ä¿®æ”¹é€Ÿç‡é™åˆ¶

ç¼–è¾‘ `deploy_with_limits.py`:
```python
@limiter.limit("10 per minute")  # æ”¹ä¸ºä½ éœ€è¦çš„å€¼
```

### ä¿®æ”¹æœ€å¤§Tokenæ•°

ç¼–è¾‘ `deploy_with_limits.py`:
```python
max_tokens = min(data.get('max_tokens', 100), 512)  # æ”¹ä¸º256æˆ–1024
```

## ğŸ“ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: å®æ—¶æ–°é—»åˆ†ç±»
```python
# å®æ—¶å¤„ç†æ–°é—»æµ
for news in news_stream:
    category = classify_news(news)
    save_to_database(news, category)
```

### åœºæ™¯2: æ‰¹é‡å†å²æ•°æ®åˆ†ç±»
```python
# æ‰¹é‡å¤„ç†å†å²æ•°æ®
from concurrent.futures import ThreadPoolExecutor

def classify_batch(news_list):
    with ThreadPoolExecutor(max_workers=3) as executor:
        results = list(executor.map(classify_news, news_list))
    return results
```

### åœºæ™¯3: APIæœåŠ¡é›†æˆ
```python
# é›†æˆåˆ°ç°æœ‰APIæœåŠ¡
@app.route('/classify', methods=['POST'])
def classify_endpoint():
    news = request.json['news']
    category = classify_news(news)
    return jsonify({'category': category})
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. é™ä½å»¶è¿Ÿ
- å‡å°‘max_tokensåˆ°100
- é™ä½temperatureåˆ°0.1
- ä½¿ç”¨ç¼“å­˜æœºåˆ¶

### 2. æé«˜ååé‡
- å¢åŠ å¹¶å‘æ•°åˆ°4ï¼ˆé£é™©ï¼šå¯èƒ½OOMï¼‰
- ä½¿ç”¨æ‰¹å¤„ç†
- éƒ¨ç½²å¤šä¸ªå®ä¾‹

### 3. èŠ‚çœæ˜¾å­˜
- å‡å°‘max_model_len
- ä½¿ç”¨4-bité‡åŒ–ï¼ˆéœ€é‡æ–°é…ç½®ï¼‰
- å®šæœŸæ¸…ç†ç¼“å­˜

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: æœåŠ¡æ— å“åº”
```bash
# æ£€æŸ¥è¿›ç¨‹
ps aux | grep deploy_with_limits

# æ£€æŸ¥ç«¯å£
netstat -tlnp | grep 8000

# é‡å¯æœåŠ¡
bash start_production.sh
```

### Q2: æ˜¾å­˜ä¸è¶³ (OOM)
```bash
# é™ä½å¹¶å‘æ•°
# ç¼–è¾‘ deploy_with_limits.py
MAX_CONCURRENT_REQUESTS = 2

# é‡å¯æœåŠ¡
bash start_production.sh
```

### Q3: è¯·æ±‚è¶…æ—¶
```bash
# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# æŸ¥çœ‹æ˜¯å¦æœ‰é˜»å¡è¯·æ±‚
curl http://localhost:8000/stats
```

### Q4: é€Ÿç‡é™åˆ¶è§¦å‘
```bash
# æŸ¥çœ‹å½“å‰é™åˆ¶
curl http://localhost:8000/health

# ä¿®æ”¹é™åˆ¶
# ç¼–è¾‘ deploy_with_limits.py
@limiter.limit("20 per minute")  # å¢åŠ é™åˆ¶
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **APIä½¿ç”¨æŒ‡å—**: `API_USAGE.md` - è¯¦ç»†çš„APIæ¥å£è¯´æ˜å’Œç¤ºä¾‹
- **å¹¶å‘é…ç½®**: `CONCURRENCY_CONFIG.md` - å¹¶å‘é™åˆ¶çš„è¯¦ç»†è¯´æ˜
- **å¿«é€Ÿå¼€å§‹**: `QUICKSTART.md` - å¿«é€Ÿå…¥é—¨æŒ‡å—
- **å®‰è£…æŒ‡å—**: `INSTALL_5090.md` - ç¯å¢ƒå®‰è£…è¯´æ˜

## ğŸ” å®‰å…¨å»ºè®®

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
1. **ä½¿ç”¨åå‘ä»£ç†** (Nginx)
2. **å¯ç”¨HTTPS**
3. **é…ç½®é˜²ç«å¢™**
4. **æ·»åŠ è®¤è¯æœºåˆ¶**
5. **æ—¥å¿—å®¡è®¡**

### ç¤ºä¾‹Nginxé…ç½®
```nginx
upstream qwen_backend {
    server 127.0.0.1:8000;
}

server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://qwen_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        
        # é€Ÿç‡é™åˆ¶
        limit_req zone=api_limit burst=5;
    }
}
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### ç¯å¢ƒä¿¡æ¯
- **Condaç¯å¢ƒ**: vllm-deploy
- **Python**: 3.10
- **PyTorch**: 2.9.0+cu128
- **Transformers**: 4.57.3
- **vLLM**: 0.13.0

### æ£€æŸ¥æ¸…å•
- [ ] GPUé©±åŠ¨æ­£å¸¸ (`nvidia-smi`)
- [ ] Condaç¯å¢ƒæ¿€æ´» (`conda activate vllm-deploy`)
- [ ] æ¨¡å‹æ–‡ä»¶å®Œæ•´ (`ls models/qwen-news-classifier-merged/`)
- [ ] ç«¯å£æœªè¢«å ç”¨ (`netstat -tlnp | grep 8000`)
- [ ] æ˜¾å­˜å……è¶³ (`nvidia-smi`)

## ğŸ‰ éƒ¨ç½²æˆåŠŸ

æ‚¨çš„Qwen3-8Bé‡‘èæ–°é—»åˆ†ç±»æ¨¡å‹å·²æˆåŠŸéƒ¨ç½²ï¼

**å½“å‰çŠ¶æ€**:
- âœ… æ¨¡å‹åŠ è½½å®Œæˆ
- âœ… 8-bité‡åŒ–è¿è¡Œ
- âœ… å¹¶å‘é™åˆ¶å·²é…ç½®
- âœ… APIæœåŠ¡æ­£å¸¸
- âœ… æµ‹è¯•é€šè¿‡

**ä¸‹ä¸€æ­¥**:
1. è¿è¡Œ `python test_api.py` æµ‹è¯•API
2. æŸ¥çœ‹ `API_USAGE.md` äº†è§£è¯¦ç»†ç”¨æ³•
3. é›†æˆåˆ°æ‚¨çš„åº”ç”¨ä¸­
4. ç›‘æ§æœåŠ¡æ€§èƒ½

ç¥ä½¿ç”¨æ„‰å¿«ï¼ğŸš€
