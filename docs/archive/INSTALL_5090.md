# RTX 5090 å®‰è£…æŒ‡å—ï¼ˆCUDA 12.8+ï¼‰

## âš ï¸ é‡è¦æç¤º

RTX 5090ï¼ˆBlackwellæ¶æ„ï¼‰**å¿…é¡»ä½¿ç”¨CUDA 12.8åŠä»¥ä¸Šç‰ˆæœ¬**

---

## å®Œæ•´å®‰è£…æµç¨‹ï¼ˆ5090æœºå™¨ï¼‰

### Step 1: æ£€æŸ¥CUDAç‰ˆæœ¬

```bash
# æ£€æŸ¥ç³»ç»ŸCUDAç‰ˆæœ¬
nvcc --version

# åº”è¯¥æ˜¾ç¤º: CUDA 12.8 æˆ–æ›´é«˜
# å¦‚æœç‰ˆæœ¬ä½äº12.8ï¼Œéœ€è¦å…ˆå‡çº§CUDAé©±åŠ¨
```

### Step 2: å®‰è£…PyTorchï¼ˆCUDA 12.8ï¼‰

```bash
# æ–¹æ¡ˆA: PyTorch 2.5+ with CUDA 12.8ï¼ˆç¨³å®šç‰ˆï¼‰
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu128

# æ–¹æ¡ˆB: PyTorch Nightlyï¼ˆæœ€æ–°æ”¯æŒï¼Œå¦‚æœ2.5.1æœ‰é—®é¢˜ï¼‰
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128

# éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.version.cuda}'); print(f'GPU: {torch.cuda.get_device_name(0)}')"

# åº”è¯¥è¾“å‡º:
# PyTorch: 2.5.1
# CUDA: 12.8
# GPU: NVIDIA GeForce RTX 5090
```

### Step 3: å®‰è£…Transformersç”Ÿæ€

```bash
pip install transformers==4.46.0 accelerate==0.34.0 peft==0.13.0 trl==0.11.0
```

### Step 4: å®‰è£…Flash Attention 2ï¼ˆé’ˆå¯¹CUDA 12.8ç¼–è¯‘ï¼‰

```bash
# âš ï¸ Flash Attentionéœ€è¦é’ˆå¯¹CUDA 12.8é‡æ–°ç¼–è¯‘

# æ–¹æ¡ˆA: ä»æºç ç¼–è¯‘ï¼ˆæ¨èï¼‰
MAX_JOBS=8 pip install flash-attn --no-build-isolation

# æ–¹æ¡ˆB: å¦‚æœç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨PyTorchåŸç”ŸSDPAï¼ˆæ€§èƒ½ç•¥é™10%ï¼‰
# ä¸å®‰è£…flash-attnï¼Œè®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨é™çº§åˆ°sdpa

# éªŒè¯
python -c "import flash_attn; print(f'Flash Attention: {flash_attn.__version__}')"
```

### Step 5: å®‰è£…å…¶ä»–ä¾èµ–

```bash
pip install datasets tensorboard wandb akshare jieba snownlp scikit-learn
```

---

## æ½œåœ¨é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: Flash Attentionç¼–è¯‘å¤±è´¥

**åŸå› **: CUDA 12.8è¾ƒæ–°ï¼ŒFlash Attentionå¯èƒ½éœ€è¦æ›´æ–°

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: ä½¿ç”¨æœ€æ–°ç‰ˆFlash Attention
pip install git+https://github.com/Dao-AILab/flash-attention.git

# æ–¹æ¡ˆ2: ä¸ä½¿ç”¨Flash Attentionï¼ˆé™çº§åˆ°SDPAï¼‰
# ä¿®æ”¹train_qwen.py:
# attn_implementation="sdpa"  # è€Œé"flash_attention_2"

# æ€§èƒ½å½±å“: é€Ÿåº¦é™ä½çº¦10-15%ï¼Œä½†ä»ç„¶å¯ç”¨
```

### é—®é¢˜2: PyTorchç‰ˆæœ¬ä¸å…¼å®¹

**ç—‡çŠ¶**: 
```
RuntimeError: CUDA error: no kernel image is available for execution on the device
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨PyTorch Nightlyï¼ˆæœ€æ–°CUDAæ”¯æŒï¼‰
pip uninstall torch torchvision torchaudio
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

### é—®é¢˜3: Transformersç‰ˆæœ¬å†²çª

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å®Œå…¨é‡è£…
pip uninstall transformers accelerate peft trl -y
pip install transformers==4.46.0 accelerate==0.34.0 peft==0.13.0 trl==0.11.0
```

---

## æ¨èé…ç½®ï¼ˆ5090ä¼˜åŒ–ï¼‰

### è®­ç»ƒé…ç½®è°ƒæ•´

```python
# train_qwen.py ä¸­çš„é…ç½®

training_args = TrainingArguments(
    # 5090æ˜¾å­˜24GBï¼Œå¯ä»¥å¼€æ›´å¤§batch
    per_device_train_batch_size=8,  # ä»4æå‡åˆ°8
    gradient_accumulation_steps=2,  # ä»4é™åˆ°2
    
    # ç²¾åº¦é…ç½®ï¼ˆ5090ä¼˜åŒ–ï¼‰
    bf16=True,          # 5090åŸç”Ÿæ”¯æŒbf16
    tf32=True,          # å¯ç”¨TF32åŠ é€Ÿ
    fp16=False,
    
    # Attentionå®ç°
    # å¦‚æœFlash Attentionå¯ç”¨
    # attn_implementation="flash_attention_2"
    # å¦‚æœä¸å¯ç”¨ï¼Œé™çº§åˆ°
    # attn_implementation="sdpa"  # PyTorchåŸç”Ÿä¼˜åŒ–
)
```

### æ¨¡å‹åŠ è½½é…ç½®

```python
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # 5090æ¨èbf16
    device_map="auto",
    trust_remote_code=True,
    
    # Attentionå®ç°ï¼ˆæ ¹æ®Flash Attentionæ˜¯å¦å¯ç”¨ï¼‰
    attn_implementation="flash_attention_2",  # ä¼˜å…ˆ
    # æˆ–é™çº§åˆ°: attn_implementation="sdpa"
    
    use_cache=False,  # è®­ç»ƒæ—¶å¿…é¡»False
)
```

---

## å®Œæ•´å®‰è£…è„šæœ¬ï¼ˆ5090ä¸“ç”¨ï¼‰

```bash
#!/bin/bash
# RTX 5090ç¯å¢ƒå®‰è£…è„šæœ¬

echo "RTX 5090 Qwenè®­ç»ƒç¯å¢ƒå®‰è£…"
echo "CUDAç‰ˆæœ¬è¦æ±‚: 12.8+"

# æ£€æŸ¥CUDAç‰ˆæœ¬
echo "æ£€æŸ¥CUDAç‰ˆæœ¬..."
nvcc --version | grep "release"

# åˆ›å»ºç¯å¢ƒ
echo "åˆ›å»ºcondaç¯å¢ƒ..."
conda create -n qwen-train python=3.10 -y
conda activate qwen-train

# å®‰è£…PyTorchï¼ˆCUDA 12.8ï¼‰
echo "å®‰è£…PyTorch 2.5+ with CUDA 12.8..."
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu128

# éªŒè¯PyTorch
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')"

# å®‰è£…Transformers
echo "å®‰è£…Transformersç”Ÿæ€..."
pip install transformers==4.46.0 accelerate==0.34.0 peft==0.13.0 trl==0.11.0

# å®‰è£…Flash Attention 2ï¼ˆå¯é€‰ï¼‰
echo "å®‰è£…Flash Attention 2..."
MAX_JOBS=8 pip install flash-attn --no-build-isolation || echo "âš ï¸ Flash Attentionå®‰è£…å¤±è´¥ï¼Œå°†ä½¿ç”¨SDPAé™çº§æ–¹æ¡ˆ"

# å®‰è£…å…¶ä»–ä¾èµ–
echo "å®‰è£…å…¶ä»–ä¾èµ–..."
pip install datasets tensorboard wandb akshare jieba snownlp scikit-learn

echo "âœ… å®‰è£…å®Œæˆï¼"
echo "éªŒè¯: python -c 'import torch; print(torch.cuda.is_available())'"
```

ä¿å­˜ä¸º `install_5090.sh`ï¼Œç„¶åè¿è¡Œï¼š
```bash
chmod +x install_5090.sh
./install_5090.sh
```

---

## æ€§èƒ½å¯¹æ¯”ï¼ˆCUDA 12.8 vs 12.1ï¼‰

| é…ç½® | CUDA 12.1 | CUDA 12.8 (5090) | æå‡ |
|------|-----------|------------------|------|
| è®¡ç®—æ€§èƒ½ | åŸºå‡† | +15% | Blackwellæ¶æ„ |
| Flash Attn 2 | æ”¯æŒ | ä¼˜åŒ–æ”¯æŒ | +5% |
| æ€»ä½“è®­ç»ƒé€Ÿåº¦ | åŸºå‡† | +20% | ç»¼åˆæå‡ |

---

## æ€»ç»“

### âœ… å…³é”®é…ç½®

**PyTorch**: `2.5.1` with CUDA `12.8`  
**Transformers**: `4.46.0`  
**Flash Attention**: é’ˆå¯¹CUDA 12.8ç¼–è¯‘ï¼ˆå¯é€‰ï¼‰

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **å¿…é¡»ä½¿ç”¨CUDA 12.8+**ï¼ˆ5090ç¡¬æ€§è¦æ±‚ï¼‰
2. Flash Attentionéœ€è¦é‡æ–°ç¼–è¯‘ï¼ˆå¦‚æœå¤±è´¥å¯é™çº§åˆ°SDPAï¼‰
3. æ¨èä½¿ç”¨bf16ï¼ˆ5090åŸç”Ÿæ”¯æŒï¼‰

### ğŸš€ å¿«é€Ÿå‘½ä»¤

```bash
# 5090æœºå™¨ä¸€é”®å®‰è£…
pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu128
pip install transformers==4.46.0 accelerate peft trl datasets
pip install flash-attn --no-build-isolation || echo "Flash Attnè·³è¿‡"
pip install tensorboard wandb akshare jieba snownlp
```

æ‰€æœ‰é…ç½®å·²æ›´æ–°ä¸ºCUDA 12.8+ï¼Œå¯ä»¥åœ¨5090ä¸Šæ­£å¸¸è¿è¡Œï¼
