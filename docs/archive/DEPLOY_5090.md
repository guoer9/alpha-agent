# RTX 5090 éƒ¨ç½²æŒ‡å—

## å®Œæ•´æ–¹æ¡ˆæ€»ç»“

### âœ… å‚è€ƒHuggingFaceå®˜æ–¹æ–¹æ¡ˆ

**å…³é”®æ”¹è¿›**:
1. âœ… ä½¿ç”¨ `SFTTrainer` (ä¸“é—¨ç”¨äºchatæ¨¡å‹)
2. âœ… ä½¿ç”¨ `tokenizer.apply_chat_template()` (è‡ªåŠ¨å¤„ç†ç‰¹æ®Štoken)
3. âœ… å®˜æ–¹æ¨èçš„è¶…å‚æ•°é…ç½®
4. âœ… Flash Attention 2 + bf16 (5090ä¼˜åŒ–)

---

## 5090æœºå™¨éƒ¨ç½²æµç¨‹

### Step 1: æ‰“åŒ…æ–‡ä»¶ï¼ˆå½“å‰æœºå™¨ï¼‰

```bash
cd /Volumes/2tb/mydata/code/Quantitative_trading/qlib_trading

# æ‰“åŒ…è®­ç»ƒä»£ç 
tar -czf qwen_training.tar.gz \
    alpha_agent/training/news_classifier/ \
    --exclude='*.pyc' \
    --exclude='__pycache__' \
    --exclude='models/*' \
    --exclude='logs/*'

# æŸ¥çœ‹å¤§å°
ls -lh qwen_training.tar.gz
```

### Step 2: ä¼ è¾“åˆ°5090

```bash
# ä½¿ç”¨scp
scp qwen_training.tar.gz user@5090-ip:/home/user/

# æˆ–ä½¿ç”¨rsyncï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
rsync -avz --progress qwen_training.tar.gz user@5090-ip:/home/user/
```

### Step 3: 5090æœºå™¨ç¯å¢ƒé…ç½®

```bash
# SSHç™»å½•5090
ssh user@5090-ip

# è§£å‹
tar -xzf qwen_training.tar.gz
cd alpha_agent/training/news_classifier

# åˆ›å»ºcondaç¯å¢ƒ
conda create -n qwen-train python=3.10 -y
conda activate qwen-train

# å®‰è£…PyTorchï¼ˆCUDA 12.1ï¼Œ5090ä¸“ç”¨ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers>=4.37.0 accelerate peft trl datasets

# å®‰è£…Flash Attention 2ï¼ˆé‡è¦ï¼åŠ é€Ÿ30%ï¼‰
pip install flash-attn --no-build-isolation

# å®‰è£…å…¶ä»–å·¥å…·
pip install tensorboard wandb akshare jieba snownlp
```

### Step 4: å‡†å¤‡æ•°æ®

```bash
# æ–¹æ¡ˆA: ä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
python download_datasets.py --sample
python prepare_data.py --generate-dataset

# æ–¹æ¡ˆB: ä¸‹è½½FinCUGEï¼ˆæ¨èï¼‰
export HF_ENDPOINT=https://hf-mirror.com  # å›½å†…åŠ é€Ÿ
python download_datasets.py --fincuge

# æ–¹æ¡ˆC: ä½¿ç”¨AkShareæ”¶é›†çœŸå®æ•°æ®
python prepare_data.py --collect --annotate --generate-dataset
```

### Step 5: å¼€å§‹è®­ç»ƒ

```bash
# æ£€æŸ¥GPU
nvidia-smi

# å¼€å§‹è®­ç»ƒ
python train_qwen.py

# åå°è¿è¡Œï¼ˆæ¨èï¼‰
nohup python train_qwen.py > train.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f train.log

# æŸ¥çœ‹TensorBoard
tensorboard --logdir=./logs --port=6006
```

---

## è®­ç»ƒé…ç½®è¯´æ˜ï¼ˆå®˜æ–¹æ¨èï¼‰

### æ¨¡å‹é€‰æ‹©

```python
# æ¨è: Qwen2.5-7B-Instruct
model_name = "Qwen/Qwen2.5-7B-Instruct"
# æ˜¾å­˜å ç”¨: ~18GB (LoRA)
# è®­ç»ƒæ—¶é—´: 2-4å°æ—¶ (1ä¸‡æ¡æ•°æ®)

# å¤‡é€‰: Qwen2.5-3B-Instruct (æ˜¾å­˜ä¸å¤Ÿæ—¶)
model_name = "Qwen/Qwen2.5-3B-Instruct"
# æ˜¾å­˜å ç”¨: ~10GB
# è®­ç»ƒæ—¶é—´: 1-2å°æ—¶
```

### LoRAé…ç½®ï¼ˆå®˜æ–¹æ¨èï¼‰

```python
lora_r = 64          # å®˜æ–¹æ¨è: 8-64
lora_alpha = 128     # é€šå¸¸æ˜¯rçš„2å€
lora_dropout = 0.05  # å®˜æ–¹æ¨è: 0.05-0.1

# Target modules: æ‰€æœ‰attentionå’ŒFFNå±‚
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
    "gate_proj", "up_proj", "down_proj"       # FFN
]
```

### è®­ç»ƒè¶…å‚æ•°ï¼ˆå®˜æ–¹æ¨èï¼‰

```python
# æ‰¹å¤§å°
per_device_train_batch_size = 4  # å®˜æ–¹æ¨èä»å°å¼€å§‹
gradient_accumulation_steps = 4  # æœ‰æ•ˆbatch_size = 16

# å­¦ä¹ ç‡
learning_rate = 5e-5  # å®˜æ–¹æ¨èèŒƒå›´: 1e-5 åˆ° 1e-4
warmup_ratio = 0.03   # å®˜æ–¹æ¨è: 0.03

# ç²¾åº¦ï¼ˆ5090ä¼˜åŒ–ï¼‰
bf16 = True   # 5090åŸç”Ÿæ”¯æŒ
tf32 = True   # å¯ç”¨TF32åŠ é€Ÿ

# Flash Attention 2
attn_implementation = "flash_attention_2"  # åŠ é€Ÿ30%
```

---

## å…³é”®æŠ€æœ¯ç‚¹ï¼ˆå®˜æ–¹æ–¹æ¡ˆï¼‰

### 1. ä½¿ç”¨SFTTrainerï¼ˆä¸æ˜¯æ™®é€šTrainerï¼‰

```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    peft_config=lora_config,  # è‡ªåŠ¨åº”ç”¨LoRA
    formatting_func=lambda x: x["messages"],  # è¿”å›messages
    max_seq_length=512,
    packing=False,  # åˆ†ç±»ä»»åŠ¡ä¸éœ€è¦packing
)
```

**ä¼˜åŠ¿**:
- è‡ªåŠ¨å¤„ç†chat template
- è‡ªåŠ¨åº”ç”¨LoRA
- ä¼˜åŒ–çš„æ•°æ®å¤„ç†

### 2. Chat Templateè‡ªåŠ¨å¤„ç†ï¼ˆä¸æ‰‹æ“ï¼‰

```python
# âœ… æ­£ç¡®æ–¹å¼ï¼ˆå®˜æ–¹æ¨èï¼‰
messages = [
    {"role": "user", "content": "åˆ†ææ–°é—»ç±»åˆ«ï¼š..."},
    {"role": "assistant", "content": "è¿™æ¡æ–°é—»å±äºï¼šè´§å¸æ”¿ç­–"}
]

# SFTTrainerè‡ªåŠ¨è°ƒç”¨
text = tokenizer.apply_chat_template(messages, tokenize=False)

# âŒ é”™è¯¯æ–¹å¼ï¼ˆä¸è¦æ‰‹æ“ï¼‰
text = f"<|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant\n{assistant_msg}<|im_end|>"
```

### 3. Gradient Checkpointingï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

```python
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

# é…ç½®
gradient_checkpointing_kwargs = {"use_reentrant": False}  # æ–°ç‰ˆæ¨è
```

---

## æ€§èƒ½é¢„ä¼°ï¼ˆRTX 5090ï¼‰

### 7Bæ¨¡å‹ + LoRA

| é…ç½® | æ‰¹å¤§å° | æ˜¾å­˜ | é€Ÿåº¦ | è®­ç»ƒæ—¶é—´ |
|------|--------|------|------|---------|
| bf16 + Flash Attn 2 | 4 | ~16GB | ~1200 tok/s | 2-3å°æ—¶ (1ä¸‡æ¡) |
| bf16 + SDPA | 4 | ~18GB | ~900 tok/s | 3-4å°æ—¶ |
| fp16 | 4 | ~18GB | ~800 tok/s | 4-5å°æ—¶ |

### 3Bæ¨¡å‹ + LoRA

| é…ç½® | æ‰¹å¤§å° | æ˜¾å­˜ | é€Ÿåº¦ | è®­ç»ƒæ—¶é—´ |
|------|--------|------|------|---------|
| bf16 + Flash Attn 2 | 8 | ~10GB | ~2000 tok/s | 1-2å°æ—¶ (1ä¸‡æ¡) |

---

## å®Œæ•´å‘½ä»¤æ¸…å•

### åœ¨5090æœºå™¨ä¸Šæ‰§è¡Œ

```bash
# 1. ç¯å¢ƒå‡†å¤‡
conda create -n qwen-train python=3.10 -y
conda activate qwen-train

# 2. å®‰è£…PyTorchï¼ˆCUDA 12.1ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 3. å®‰è£…ä¾èµ–
pip install transformers accelerate peft trl datasets
pip install flash-attn --no-build-isolation
pip install tensorboard wandb akshare jieba snownlp

# 4. å‡†å¤‡æ•°æ®ï¼ˆé€‰æ‹©ä¸€ç§ï¼‰
# æ–¹æ¡ˆA: ç¤ºä¾‹æ•°æ®é›†ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
python download_datasets.py --sample
python prepare_data.py --generate-dataset

# æ–¹æ¡ˆB: FinCUGEæ•°æ®é›†ï¼ˆæ¨èï¼‰
export HF_ENDPOINT=https://hf-mirror.com
python download_datasets.py --fincuge

# 5. å¼€å§‹è®­ç»ƒ
python train_qwen.py

# 6. ç›‘æ§è®­ç»ƒ
tensorboard --logdir=./logs --port=6006
# æµè§ˆå™¨è®¿é—®: http://5090-ip:6006

# 7. è®­ç»ƒå®Œæˆåæµ‹è¯•
python inference_qwen.py
```

---

## æ•…éšœæ’æŸ¥

### Q1: Flash Attentionå®‰è£…å¤±è´¥

```bash
# æ–¹æ¡ˆ1: ä½¿ç”¨é¢„ç¼–è¯‘wheel
pip install flash-attn --no-build-isolation

# æ–¹æ¡ˆ2: ä»æºç ç¼–è¯‘ï¼ˆéœ€è¦æ—¶é—´ï¼‰
pip install flash-attn --no-build-isolation --no-cache-dir

# æ–¹æ¡ˆ3: ä¸ä½¿ç”¨Flash Attentionï¼ˆé™çº§ï¼‰
# ä¿®æ”¹train_qwen.py: use_flash_attention=False
```

### Q2: æ˜¾å­˜ä¸è¶³

```bash
# æ–¹æ¡ˆ1: å‡å°batch size
per_device_train_batch_size = 2
gradient_accumulation_steps = 8

# æ–¹æ¡ˆ2: ä½¿ç”¨3Bæ¨¡å‹
model_name = "Qwen/Qwen2.5-3B-Instruct"

# æ–¹æ¡ˆ3: ä½¿ç”¨4bité‡åŒ–
load_in_4bit = True
```

### Q3: æ•°æ®é›†ä¸‹è½½å¤±è´¥

```bash
# ä½¿ç”¨HuggingFaceé•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–æ‰‹åŠ¨ä¸‹è½½ååŠ è½½
dataset = load_dataset('json', data_files='local_file.jsonl')
```

---

## è®­ç»ƒåé›†æˆ

### åœ¨åŸç³»ç»Ÿä¸­ä½¿ç”¨å¾®è°ƒæ¨¡å‹

```python
# alpha_agent/data_sources/news_processor.py

from alpha_agent.training.news_classifier.inference_qwen import QwenNewsClassifier

class NewsProcessor:
    def __init__(self, use_qwen: bool = True):
        if use_qwen:
            self.classifier = QwenNewsClassifier(
                model_path="./models/qwen-news-classifier",
                device="cuda" if torch.cuda.is_available() else "cpu",
            )
    
    def _extract_themes(self, text: str):
        """ä½¿ç”¨Qwenæ¨¡å‹æå–ä¸»é¢˜"""
        result = self.classifier.classify(text)
        return [result['category']]
```

---

## æ€»ç»“

### âœ… å·²å®Œæˆ

1. âœ… å‚è€ƒHuggingFaceå®˜æ–¹æ–¹æ¡ˆä¼˜åŒ–è®­ç»ƒè„šæœ¬
2. âœ… ä½¿ç”¨SFTTrainer + apply_chat_template
3. âœ… 5090ä¼˜åŒ–é…ç½®ï¼ˆbf16 + Flash Attn 2ï¼‰
4. âœ… å®Œæ•´çš„requirements.txt
5. âœ… ç¤ºä¾‹æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼ˆ23æ¡ï¼‰
6. âœ… éƒ¨ç½²æ–‡æ¡£

### ğŸš€ ä¸‹ä¸€æ­¥

**åœ¨5090æœºå™¨ä¸Š**:
```bash
# 1. è§£å‹ä»£ç 
tar -xzf qwen_training.tar.gz

# 2. å®‰è£…ç¯å¢ƒ
conda create -n qwen-train python=3.10 -y
conda activate qwen-train
pip install -r requirements.txt

# 3. å‡†å¤‡æ•°æ®
python download_datasets.py --sample

# 4. å¼€å§‹è®­ç»ƒ
python train_qwen.py
```

**é¢„è®¡**: 2-4å°æ—¶å®Œæˆè®­ç»ƒï¼

æ‰€æœ‰ä»£ç å·²å°±ç»ªï¼Œå¯ä»¥ç›´æ¥è¿ç§»åˆ°5090è¿è¡Œï¼
